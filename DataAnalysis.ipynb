{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "import copy\n",
    "# named tuple has methods like _asdict()\n",
    "ROOT = \"*\"\n",
    "DepSample = namedtuple('DepSample', 'idx, token, pos, head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"1\tPierre\t_\tNNP\t_\t_\t2\tNAME\t_\t_\"\n",
    "example = example.split('\\t')\n",
    "example = DepSample(example[0], example[1], example[3], example[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_sample_generator(path_to_file):\n",
    "    \"\"\"\n",
    "    This function generates samples, such that every sample is a list\n",
    "    ordered by the tokens' counter (first column).\n",
    "    :param: path_to_file: string to the location of the file tor read from (str)\n",
    "    :return: sample (list of DepSample)\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(path_to_file), \"File does not exist\"\n",
    "    root = DepSample(0, ROOT, ROOT, 0)\n",
    "    with open(path_to_file) as fp:\n",
    "        sample = [root]\n",
    "        for line in fp:\n",
    "            if not line.rstrip():\n",
    "                yield sample\n",
    "                sample = [root]\n",
    "            else:\n",
    "                ls = line.rstrip().split('\\t')\n",
    "#                 print(ls)\n",
    "                sample.append(DepSample(int(ls[0]), ls[1], ls[3], int(ls[6])))\n",
    "        if len(sample) > 1:\n",
    "            yield sample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = './data/train.labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validation(path_to_file, valid_amount=0.2):\n",
    "    \"\"\"\n",
    "    This functions takes a train dataset and splits it to trainining\n",
    "    and validation sets accoording to `valid_amount`.\n",
    "    :param: path_to_file: path to file containing the dataset (str)\n",
    "    :param: valid_amount: percentage of samples to take for validation (float)\n",
    "    :return: train_file_path: path to file containing the training samples (str)\n",
    "    :return: valid_file_path: path to file containing the validation samples (str)\n",
    "    \"\"\"\n",
    "    path_train_file = path_to_file + \".train.labeled\"\n",
    "    path_valid_file = path_to_file + \".valid.labeled\"\n",
    "    # count samples\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    total_samples = 0\n",
    "    for _ in samp_gen:\n",
    "        total_samples += 1\n",
    "    print(\"total samples \", total_samples)\n",
    "    buffer = []\n",
    "    num_validation = int(valid_amount * total_samples)\n",
    "    num_training = total_samples - num_validation\n",
    "    taken_for_training = 0\n",
    "    t_file = open(path_train_file, 'w')\n",
    "    v_file = open(path_valid_file, 'w')\n",
    "    with open(path_to_file) as fp:\n",
    "        sample = []\n",
    "        for line in fp:\n",
    "            if not line.rstrip():\n",
    "                if taken_for_training < num_training:\n",
    "                    for l in sample:\n",
    "                        t_file.write(l)\n",
    "                    t_file.write('\\n')\n",
    "                    taken_for_training += 1\n",
    "                else:\n",
    "                    for l in sample:\n",
    "                        v_file.write(l)\n",
    "                    v_file.write('\\n')\n",
    "                sample = []\n",
    "            else:\n",
    "                sample.append(line)\n",
    "                \n",
    "        if taken_for_training < num_training:\n",
    "            for l in sample:\n",
    "                t_file.write(l)\n",
    "            t_file.write('\\n')\n",
    "            taken_for_training += 1\n",
    "        else:\n",
    "            for l in sample:\n",
    "                v_file.write(l)\n",
    "            v_file.write('\\n')\n",
    "    t_file.close()\n",
    "    v_file.close()\n",
    "    print(\"num training: \", num_training, \" saved @ \", path_train_file)\n",
    "    print(\"num validation: \", num_validation, \" saved @ \", path_valid_file)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples  5000\n",
      "num training:  4000  saved @  ./data/train.labeled.train.labeled\n",
      "num validation:  1000  saved @  ./data/train.labeled.valid.labeled\n"
     ]
    }
   ],
   "source": [
    "split_train_validation(path_to_file, valid_amount=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_file = './data/train.labeled.train.labeled'\n",
    "path_to_Valid_file = './data/train.labeled.valid.labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DepSample(idx=0, token='*', pos='*', head=0), DepSample(idx=1, token='Alan', pos='NNP', head=2), DepSample(idx=2, token='Spoon', pos='NNP', head=9), DepSample(idx=3, token=',', pos=',', head=2), DepSample(idx=4, token='recently', pos='RB', head=5), DepSample(idx=5, token='named', pos='VBN', head=2), DepSample(idx=6, token='Newsweek', pos='NNP', head=7), DepSample(idx=7, token='president', pos='NN', head=5), DepSample(idx=8, token=',', pos=',', head=2), DepSample(idx=9, token='said', pos='VBD', head=0), DepSample(idx=10, token='Newsweek', pos='NNP', head=13), DepSample(idx=11, token=\"'s\", pos='POS', head=10), DepSample(idx=12, token='ad', pos='NN', head=13), DepSample(idx=13, token='rates', pos='NNS', head=14), DepSample(idx=14, token='would', pos='MD', head=9), DepSample(idx=15, token='increase', pos='VB', head=14), DepSample(idx=16, token='5', pos='CD', head=17), DepSample(idx=17, token='%', pos='NN', head=15), DepSample(idx=18, token='in', pos='IN', head=15), DepSample(idx=19, token='January', pos='NNP', head=18), DepSample(idx=20, token='.', pos='.', head=9)]\n"
     ]
    }
   ],
   "source": [
    "samp_gen = dep_sample_generator(path_to_file=path_to_train_file)\n",
    "for s_i, s in enumerate(samp_gen):\n",
    "    if s_i == 100:\n",
    "        print(s)\n",
    "#     if s_i > 0:\n",
    "#         break\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Sample\n",
    "\n",
    "number of tokens in a sample = `sample[-1].idx`\n",
    "\n",
    "POS tags = `[s.pos for s in sample]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample length:  4\n",
      "POS tags:  ['*', 'WP', 'VBZ', 'JJ', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_len = s[-1].idx\n",
    "s_tags = [l.pos for l in s]\n",
    "print(\"sample length: \", sample_len)\n",
    "print(\"POS tags: \", s_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "### Unigram\n",
    "* (head_word, head_pos)\n",
    "* (head_word)\n",
    "* (head_pos)\n",
    "* (child_word, child_pos)\n",
    "* (child_word)\n",
    "* (child_pos)\n",
    "\n",
    "### Bigram\n",
    "* (head_word, head_pos, child_word, child_pos)\n",
    "* (head_pos, child_word, child_pos)\n",
    "* (head_word, child_word, child_pos)\n",
    "* (head_word, head_pos, child_pos)\n",
    "* (head_word, head_pos, child_word)\n",
    "* (head_word, child_word)\n",
    "* (head_pos, child_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_hist_dict(path_to_file, save_to_file=False):\n",
    "    \"\"\"\n",
    "    This function generates histogram of of the tokens in the dataset.\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: save_to_file: whether or not to save the dictionary on disk (bool)\n",
    "    :return: word_hist: OrderedDict word->word_count\n",
    "    \"\"\"\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    word_hist = {}\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT:\n",
    "                continue\n",
    "            if word_hist.get(s.token):\n",
    "                word_hist[s.token] += 1\n",
    "            else:\n",
    "                word_hist[s.token] = 1\n",
    "    word_hist = OrderedDict(sorted(word_hist.items(), key=lambda t: -t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".word.hist\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(word_hist, fp)\n",
    "        print(\"word histogram dictionary saved @ \", path)\n",
    "    return word_hist\n",
    "\n",
    "\"\"\"\n",
    "UNIGRAMS\n",
    "\"\"\"\n",
    "\n",
    "def generate_hw_hp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            # ignore ROOT\n",
    "            if s.token == ROOT or (word_hist.get(sample[s.head].token) \\\n",
    "                                   and word_hist[sample[s.head].token] < word_threshold):\n",
    "                continue\n",
    "            if sample[s.head].token == ROOT:\n",
    "                continue\n",
    "            feats = [(sample[s.head].token, sample[s.head].pos),\n",
    "                     (sample[s.head].token),\n",
    "                     (sample[s.head].pos)]\n",
    "            for feat in feats:\n",
    "                if hw_hp_feat_dict.get(feat) is None:\n",
    "                    hw_hp_feat_dict[feat] = current_idx\n",
    "                    current_idx += 1\n",
    "    print(\"total (head_word, head_pos), (head_word), (head_pos) features: \", current_idx)\n",
    "    hw_hp_feat_dict = OrderedDict(sorted(hw_hp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos), (head_word), (head_pos) features dictionary @ \", path)\n",
    "    return hw_hp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_hp_feat_indices(sample, hw_hp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_hp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_hp_dict.get((sample[s.head].token, sample[s.head].pos)):\n",
    "            idx = hw_hp_dict.get((sample[s.head].token, sample[s.head].pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if hw_hp_dict.get((sample[s.head].token)):\n",
    "            idx = hw_hp_dict.get((sample[s.head].token))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if hw_hp_dict.get((sample[s.head].pos)):\n",
    "            idx = hw_hp_dict.get((sample[s.head].pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def generate_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feats = [(s.token, s.pos),\n",
    "                     (s.token),\n",
    "                     (s.pos)]\n",
    "            for feat in feats:\n",
    "                if cw_cp_feat_dict.get(feat) is None:\n",
    "                    cw_cp_feat_dict[feat] = current_idx\n",
    "                    current_idx += 1\n",
    "    print(\"total (child_word, child_pos), (child_word), (child_pos) features: \", current_idx)\n",
    "    cw_cp_feat_dict = OrderedDict(sorted(cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(cw_cp_feat_dict, fp)\n",
    "        print(\"saved (child_word, child_pos), (child_word), (child_pos) features dictionary @ \", path)\n",
    "    return cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_cw_cp_feat_indices(sample, cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if cw_cp_dict.get((s.token, s.pos)):\n",
    "            idx = cw_cp_dict.get((s.token, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if cw_cp_dict.get((s.token)):\n",
    "            idx = cw_cp_dict.get((s.token))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if cw_cp_dict.get((s.pos)):\n",
    "            idx = cw_cp_dict.get((s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def generate_unigram_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: unigram_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    hw_hp_dict = generate_hw_hp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                          save_to_file=save_to_file, word_hist=word_hist)\n",
    "    cw_cp_dict = generate_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                          save_to_file=save_to_file, word_hist=word_hist)\n",
    "    print(\"total unigrams features: \", len(hw_hp_dict) + len(cw_cp_dict))\n",
    "    return hw_hp_dict, cw_cp_dict\n",
    "\n",
    "\n",
    "def extract_unigram_feat_indices(sample, unigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: unigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    num_hw_hp_feats = len(unigram_dict[0])\n",
    "    num_cw_cp_feats = len(unigram_dict[1])\n",
    "    current_num_features = 0\n",
    "    hw_hp_ind = extract_hw_hp_feat_indices(s, unigram_dict[0])\n",
    "    current_num_features += num_hw_hp_feats\n",
    "    cw_cp_ind = extract_cw_cp_feat_indices(s, unigram_dict[1])\n",
    "    unigram_indices = copy.deepcopy(hw_hp_ind)\n",
    "    for item in cw_cp_ind.items():\n",
    "        unigram_indices[current_num_features + item[0]] = item[1]\n",
    "    current_num_features += num_cw_cp_feats\n",
    "    return OrderedDict(sorted(unigram_indices.items(), key=lambda t: t[0]))\n",
    "\n",
    "\"\"\"\n",
    "TRIGRAMS\n",
    "\"\"\"\n",
    "\n",
    "def generate_hw_hp_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, sample[s.head].pos, s.token, s.pos)\n",
    "            if hw_hp_cw_cp_feat_dict.get(feat) is None:\n",
    "                hw_hp_cw_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, head_pos, child_word, child_pos) features: \", current_idx)\n",
    "    hw_hp_cw_cp_feat_dict = OrderedDict(sorted(hw_hp_cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp_cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_cw_cp_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos, child_word, child_pos) features dictionary @ \", path)\n",
    "    return hw_hp_cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hp_cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hp_cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].pos, s.token, s.pos)\n",
    "            if hp_cw_cp_feat_dict.get(feat) is None:\n",
    "                hp_cw_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_pos, child_word, child_pos) features: \", current_idx)\n",
    "    hp_cw_cp_feat_dict = OrderedDict(sorted(hp_cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hp_cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hp_cw_cp_feat_dict, fp)\n",
    "        print(\"saved (head_pos, child_word, child_pos) features dictionary @ \", path)\n",
    "    return hp_cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def generate_hw_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, child_word, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, s.token, s.pos)\n",
    "            if hw_cw_cp_feat_dict.get(feat) is None:\n",
    "                hw_cw_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, child_word, child_pos) features: \", current_idx)\n",
    "    hw_cw_cp_feat_dict = OrderedDict(sorted(hw_cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_cw_cp_feat_dict, fp)\n",
    "        print(\"saved (head_word, child_word, child_pos) features dictionary @ \", path)\n",
    "    return hw_cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, sample[s.head].pos, s.pos)\n",
    "            if hw_hp_cp_feat_dict.get(feat) is None:\n",
    "                hw_hp_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, head_pos, child_pos) features: \", current_idx)\n",
    "    hw_hp_cp_feat_dict = OrderedDict(sorted(hw_hp_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_cp_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos, child_pos) features dictionary @ \", path)\n",
    "    return hw_hp_cp_feat_dict\n",
    "\n",
    "\n",
    "def generate_hw_hp_cw_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_word)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_cw_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_cw_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, sample[s.head].pos, s.token)\n",
    "            if hw_hp_cw_feat_dict.get(feat) is None:\n",
    "                hw_hp_cw_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, head_pos, child_word) features: \", current_idx)\n",
    "    hw_hp_cw_feat_dict = OrderedDict(sorted(hw_hp_cw_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp_cw.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_cw_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos, child_word) features dictionary @ \", path)\n",
    "    return hw_hp_cw_feat_dict\n",
    "\n",
    "\n",
    "def generate_hw_cw_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, child_word)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_cw_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_cw_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, s.token)\n",
    "            if hw_cw_feat_dict.get(feat) is None:\n",
    "                hw_cw_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, child_word) features: \", current_idx)\n",
    "    hw_cw_feat_dict = OrderedDict(sorted(hw_cw_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_cw.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_cw_feat_dict, fp)\n",
    "        print(\"saved (head_word, child_word) features dictionary @ \", path)\n",
    "    return hw_cw_feat_dict\n",
    "\n",
    "\n",
    "def generate_hp_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hp_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hp_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].pos, s.pos)\n",
    "            if hp_cp_feat_dict.get(feat) is None:\n",
    "                hp_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_pos, child_pos) features: \", current_idx)\n",
    "    hp_cp_feat_dict = OrderedDict(sorted(hp_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hp_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hp_cp_feat_dict, fp)\n",
    "        print(\"saved (head_pos, child_pos) features dictionary @ \", path)\n",
    "    return hp_cp_feat_dict\n",
    "\n",
    "\n",
    "def generate_trigram_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_word)\n",
    "    * (head_word, child_word)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: trigram_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    num_features = 0\n",
    "    hw_hp_cw_cp_dict = generate_hw_hp_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                                     save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cw_cp_dict)\n",
    "    hp_cw_cp_dict = generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cw_cp_dict)\n",
    "    hw_cw_cp_dict = generate_hw_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_cw_cp_dict)\n",
    "    hw_hp_cp_dict = generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cp_dict)\n",
    "    hw_hp_cw_dict = generate_hw_hp_cw_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cw_dict)\n",
    "    hw_cw_dict = generate_hw_cw_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_cw_dict)\n",
    "    hp_cp_dict = generate_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cp_dict)\n",
    "    print(\"total trigrams features: \", num_features)\n",
    "    return hw_hp_cw_cp_dict, hp_cw_cp_dict, hw_cw_cp_dict, hw_hp_cp_dict, hw_hp_cw_dict, hw_cw_dict, hp_cp_dict\n",
    "\n",
    "\n",
    "def generate_trigram_feat_dict_minimal(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: trigram_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    num_features = 0\n",
    "    hp_cw_cp_dict = generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cw_cp_dict)\n",
    "    hw_hp_cp_dict = generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cp_dict)\n",
    "    \n",
    "    hp_cp_dict = generate_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cp_dict)\n",
    "    print(\"total trigrams features: \", num_features)\n",
    "    return hp_cw_cp_dict, hw_hp_cp_dict, hp_cp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14162\n"
     ]
    }
   ],
   "source": [
    "word_hist = generate_word_hist_dict(path_to_file)\n",
    "print(len(word_hist))\n",
    "# print(word_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total (head_pos, child_word, child_pos) features:  31314\n",
      "total (head_word, head_pos, child_pos) features:  33936\n",
      "total (head_pos, child_pos) features:  749\n",
      "total trigrams features:  65999\n"
     ]
    }
   ],
   "source": [
    "# unigram_feat_dict = generate_unigram_feat_dict(path_to_file)\n",
    "# hw_hp_dict = generate_hw_hp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None)\n",
    "# cw_cp_dict = generate_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None)\n",
    "# hw_hp_cw_cp_dict = generate_hw_hp_cw_cp_feat_dict(path_to_file, word_threshold=2)\n",
    "# hp_cw_cp_dict = generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=0)\n",
    "# hw_cw_cp_dict = generate_hw_cw_cp_feat_dict(path_to_file, word_threshold=4)\n",
    "# hw_hp_cp_dict = generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=2)\n",
    "# hw_hp_cw_dict = generate_hw_hp_cw_feat_dict(path_to_file, word_threshold=1)\n",
    "# hw_cw_dict = generate_hw_cw_feat_dict(path_to_file, word_threshold=1)\n",
    "# hp_cp_dict = generate_hp_cp_feat_dict(path_to_file, word_threshold=1)\n",
    "# trigram_feat_dict = generate_trigram_feat_dict(path_to_file, word_threshold=0)\n",
    "trigram_feat_dict_minimal = generate_trigram_feat_dict_minimal(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('NNP', 'NNP'), 0),\n",
       "             (('MD', 'NNP'), 1),\n",
       "             (('NNP', ','), 2),\n",
       "             (('NNS', 'CD'), 3),\n",
       "             (('JJ', 'NNS'), 4),\n",
       "             (('NNP', 'JJ'), 5),\n",
       "             (('*', 'MD'), 6),\n",
       "             (('MD', 'VB'), 7),\n",
       "             (('NN', 'DT'), 8),\n",
       "             (('VB', 'NN'), 9),\n",
       "             (('VB', 'IN'), 10),\n",
       "             (('NN', 'JJ'), 11),\n",
       "             (('IN', 'NN'), 12),\n",
       "             (('VB', 'NNP'), 13),\n",
       "             (('NNP', 'CD'), 14),\n",
       "             (('MD', '.'), 15),\n",
       "             (('VBZ', 'NNP'), 16),\n",
       "             (('*', 'VBZ'), 17),\n",
       "             (('VBZ', 'NN'), 18),\n",
       "             (('NN', 'IN'), 19),\n",
       "             (('IN', 'NNP'), 20),\n",
       "             (('NN', 'NNP'), 21),\n",
       "             (('NN', 'VBG'), 22),\n",
       "             (('NNP', 'NN'), 23),\n",
       "             (('VBZ', '.'), 24),\n",
       "             (('VBD', 'NNP'), 25),\n",
       "             (('JJ', 'CC'), 26),\n",
       "             (('CC', 'NN'), 27),\n",
       "             (('*', 'VBD'), 28),\n",
       "             (('VBD', 'VBN'), 29),\n",
       "             (('VBN', 'NN'), 30),\n",
       "             (('VBD', '.'), 31),\n",
       "             (('VBN', 'RB'), 32),\n",
       "             (('NN', 'VBN'), 33),\n",
       "             (('VBN', 'TO'), 34),\n",
       "             (('TO', 'VB'), 35),\n",
       "             (('NNS', 'NNP'), 36),\n",
       "             (('NNS', 'NN'), 37),\n",
       "             (('VB', 'NNS'), 38),\n",
       "             (('VBD', 'VBZ'), 39),\n",
       "             (('VBZ', 'VBN'), 40),\n",
       "             (('IN', 'NNS'), 41),\n",
       "             (('NNS', 'VBN'), 42),\n",
       "             (('TO', 'PRP'), 43),\n",
       "             (('CD', 'RBR'), 44),\n",
       "             (('CD', 'IN'), 45),\n",
       "             (('VBN', 'IN'), 46),\n",
       "             (('VBD', ','), 47),\n",
       "             (('VBD', 'NNS'), 48),\n",
       "             (('NN', 'NN'), 49),\n",
       "             (('NN', ','), 50),\n",
       "             (('JJ', 'RB'), 51),\n",
       "             (('VBZ', 'JJ'), 52),\n",
       "             (('VBZ', 'IN'), 53),\n",
       "             (('VBZ', 'PRP'), 54),\n",
       "             (('IN', 'VBZ'), 55),\n",
       "             (('NNS', 'DT'), 56),\n",
       "             (('VBZ', 'NNS'), 57),\n",
       "             (('VBZ', ','), 58),\n",
       "             (('NNS', 'RB'), 59),\n",
       "             (('NNS', 'JJ'), 60),\n",
       "             (('VBG', 'NNS'), 61),\n",
       "             (('NNS', 'TO'), 62),\n",
       "             (('IN', 'VBG'), 63),\n",
       "             (('VBP', 'WDT'), 64),\n",
       "             (('NNS', 'VBP'), 65),\n",
       "             (('VBP', 'RP'), 66),\n",
       "             (('VBP', 'JJ'), 67),\n",
       "             (('JJ', 'JJ'), 68),\n",
       "             (('VBZ', 'WDT'), 69),\n",
       "             (('NN', 'VBZ'), 70),\n",
       "             (('VBD', 'VBG'), 71),\n",
       "             (('VBG', 'NN'), 72),\n",
       "             (('VBG', 'IN'), 73),\n",
       "             (('NNS', 'PRP$'), 74),\n",
       "             (('IN', 'CD'), 75),\n",
       "             (('VBP', 'IN'), 76),\n",
       "             (('IN', 'VBD'), 77),\n",
       "             (('DT', 'RBR'), 78),\n",
       "             (('DT', 'IN'), 79),\n",
       "             (('VBP', ','), 80),\n",
       "             (('NNS', 'JJS'), 81),\n",
       "             (('VBP', 'NNS'), 82),\n",
       "             (('*', 'VBP'), 83),\n",
       "             (('NN', 'POS'), 84),\n",
       "             (('NNP', 'IN'), 85),\n",
       "             (('JJ', 'TO'), 86),\n",
       "             (('VB', 'TO'), 87),\n",
       "             (('TO', 'NN'), 88),\n",
       "             (('VBP', '.'), 89),\n",
       "             (('VBD', 'NN'), 90),\n",
       "             (('VBD', '``'), 91),\n",
       "             (('VBZ', 'DT'), 92),\n",
       "             (('VBP', 'PRP'), 93),\n",
       "             (('VBP', 'VBG'), 94),\n",
       "             (('IN', 'IN'), 95),\n",
       "             (('VBD', 'IN'), 96),\n",
       "             (('VBZ', 'EX'), 97),\n",
       "             (('VBZ', 'RB'), 98),\n",
       "             (('VBZ', \"''\"), 99),\n",
       "             (('NNP', 'DT'), 100),\n",
       "             (('NNP', 'CC'), 101),\n",
       "             (('CC', 'NNS'), 102),\n",
       "             (('VBD', 'WP'), 103),\n",
       "             (('NNS', 'VBD'), 104),\n",
       "             (('VBD', 'JJ'), 105),\n",
       "             (('JJ', 'IN'), 106),\n",
       "             (('NNS', 'IN'), 107),\n",
       "             (('VBD', 'VBP'), 108),\n",
       "             (('VBP', 'NN'), 109),\n",
       "             (('IN', 'VBP'), 110),\n",
       "             (('VBD', \"''\"), 111),\n",
       "             (('NNP', 'POS'), 112),\n",
       "             (('CC', 'NNP'), 113),\n",
       "             (('VBD', 'VBD'), 114),\n",
       "             (('IN', '``'), 115),\n",
       "             (('IN', \"''\"), 116),\n",
       "             (('CD', 'DT'), 117),\n",
       "             (('CD', 'JJ'), 118),\n",
       "             (('VBN', 'CC'), 119),\n",
       "             (('CC', 'VBN'), 120),\n",
       "             (('IN', 'TO'), 121),\n",
       "             (('TO', 'CD'), 122),\n",
       "             (('CD', 'CD'), 123),\n",
       "             (('VBD', 'RB'), 124),\n",
       "             (('VBP', 'CD'), 125),\n",
       "             (('VBP', 'VBN'), 126),\n",
       "             (('VBN', ':'), 127),\n",
       "             (('NN', 'NNS'), 128),\n",
       "             (('NNS', 'VBG'), 129),\n",
       "             (('VBG', 'CD'), 130),\n",
       "             (('NN', 'CC'), 131),\n",
       "             (('JJR', 'RB'), 132),\n",
       "             (('VBD', 'JJR'), 133),\n",
       "             (('JJR', 'IN'), 134),\n",
       "             (('IN', 'VBN'), 135),\n",
       "             (('IN', 'DT'), 136),\n",
       "             (('IN', 'PRP'), 137),\n",
       "             (('VBP', 'WP'), 138),\n",
       "             (('DT', 'VBP'), 139),\n",
       "             (('VBZ', 'TO'), 140),\n",
       "             (('JJS', 'DT'), 141),\n",
       "             (('VB', 'JJS'), 142),\n",
       "             (('JJS', 'IN'), 143),\n",
       "             (('VBD', 'PRP'), 144),\n",
       "             (('NN', 'TO'), 145),\n",
       "             (('MD', 'NN'), 146),\n",
       "             (('MD', 'RB'), 147),\n",
       "             (('VBD', 'MD'), 148),\n",
       "             (('VB', 'DT'), 149),\n",
       "             (('IN', 'MD'), 150),\n",
       "             (('RB', 'RBR'), 151),\n",
       "             (('VB', 'RB'), 152),\n",
       "             (('RB', 'IN'), 153),\n",
       "             (('NNS', 'CC'), 154),\n",
       "             (('VBZ', 'CD'), 155),\n",
       "             (('NNS', 'VBZ'), 156),\n",
       "             (('VBZ', 'VB'), 157),\n",
       "             (('NN', 'JJR'), 158),\n",
       "             (('NNS', ','), 159),\n",
       "             (('IN', 'JJ'), 160),\n",
       "             (('VBZ', 'VBG'), 161),\n",
       "             (('VBG', 'TO'), 162),\n",
       "             (('TO', 'NNP'), 163),\n",
       "             (('NNS', 'RBR'), 164),\n",
       "             (('VBP', 'CC'), 165),\n",
       "             (('CC', 'VBP'), 166),\n",
       "             (('DT', 'RB'), 167),\n",
       "             (('MD', 'IN'), 168),\n",
       "             (('MD', ','), 169),\n",
       "             (('MD', 'NNS'), 170),\n",
       "             (('VB', 'VBN'), 171),\n",
       "             (('VBD', 'WDT'), 172),\n",
       "             (('NN', 'VBD'), 173),\n",
       "             (('VBD', 'WRB'), 174),\n",
       "             (('VBD', 'RP'), 175),\n",
       "             (('VBD', 'CC'), 176),\n",
       "             (('CC', 'VBD'), 177),\n",
       "             (('NNS', '``'), 178),\n",
       "             (('NNS', \"''\"), 179),\n",
       "             (('IN', 'RB'), 180),\n",
       "             (('VBD', 'DT'), 181),\n",
       "             (('VBP', '``'), 182),\n",
       "             (('VBP', 'TO'), 183),\n",
       "             (('NN', 'PRP$'), 184),\n",
       "             (('VBD', 'TO'), 185),\n",
       "             (('NN', 'CD'), 186),\n",
       "             (('CC', 'IN'), 187),\n",
       "             (('NNS', 'NNS'), 188),\n",
       "             (('NNS', 'POS'), 189),\n",
       "             (('TO', 'NNS'), 190),\n",
       "             (('NNS', 'JJR'), 191),\n",
       "             (('TO', 'RB'), 192),\n",
       "             (('VBP', 'RB'), 193),\n",
       "             (('NNS', ':'), 194),\n",
       "             (('MD', 'VBD'), 195),\n",
       "             (('MD', '``'), 196),\n",
       "             (('VB', 'RP'), 197),\n",
       "             (('VB', \"''\"), 198),\n",
       "             (('IN', '$'), 199),\n",
       "             (('$', 'CD'), 200),\n",
       "             (('NN', 'JJS'), 201),\n",
       "             (('TO', '$'), 202),\n",
       "             (('VB', 'CC'), 203),\n",
       "             (('CC', 'VB'), 204),\n",
       "             (('NN', 'RB'), 205),\n",
       "             (('RB', 'CD'), 206),\n",
       "             (('JJR', 'NN'), 207),\n",
       "             (('IN', 'JJR'), 208),\n",
       "             (('VBZ', 'CC'), 209),\n",
       "             (('CC', 'VBZ'), 210),\n",
       "             (('VBG', ','), 211),\n",
       "             (('VBG', 'VBZ'), 212),\n",
       "             (('VBD', ':'), 213),\n",
       "             (('NNP', 'VBZ'), 214),\n",
       "             (('NNP', 'VBD'), 215),\n",
       "             (('$', 'NN'), 216),\n",
       "             (('$', ','), 217),\n",
       "             (('$', 'CC'), 218),\n",
       "             (('CC', '$'), 219),\n",
       "             (('NNP', 'PRP$'), 220),\n",
       "             (('NNP', '.'), 221),\n",
       "             (('NNP', 'VBN'), 222),\n",
       "             (('$', 'IN'), 223),\n",
       "             (('IN', 'JJS'), 224),\n",
       "             (('MD', 'PRP'), 225),\n",
       "             (('VBZ', 'WRB'), 226),\n",
       "             (('VBP', 'VBZ'), 227),\n",
       "             (('*', 'NN'), 228),\n",
       "             (('NN', '.'), 229),\n",
       "             (('NN', ':'), 230),\n",
       "             (('IN', ','), 231),\n",
       "             (('VB', 'PRP'), 232),\n",
       "             (('VBN', ','), 233),\n",
       "             (('VBP', 'WRB'), 234),\n",
       "             (('VBN', 'VBP'), 235),\n",
       "             (('CC', 'CD'), 236),\n",
       "             (('RB', 'NN'), 237),\n",
       "             (('VBG', 'RB'), 238),\n",
       "             (('VBD', 'CD'), 239),\n",
       "             (('JJ', ':'), 240),\n",
       "             (('VBG', 'VBP'), 241),\n",
       "             (('VBP', 'JJR'), 242),\n",
       "             (('NNP', ':'), 243),\n",
       "             (('JJ', 'DT'), 244),\n",
       "             (('JJ', 'CD'), 245),\n",
       "             (('JJ', 'NN'), 246),\n",
       "             (('$', 'RB'), 247),\n",
       "             (('VBN', 'NNS'), 248),\n",
       "             (('VBG', 'CC'), 249),\n",
       "             (('CC', 'VBG'), 250),\n",
       "             (('VB', '$'), 251),\n",
       "             (('VB', 'JJR'), 252),\n",
       "             (('MD', ':'), 253),\n",
       "             (('MD', 'VBZ'), 254),\n",
       "             (('VBZ', '$'), 255),\n",
       "             (('VB', ','), 256),\n",
       "             (('RB', 'RB'), 257),\n",
       "             (('VBD', '$'), 258),\n",
       "             (('CD', ','), 259),\n",
       "             (('CD', 'NN'), 260),\n",
       "             (('VBZ', 'WP'), 261),\n",
       "             (('VBZ', 'VBZ'), 262),\n",
       "             (('VBZ', 'VBP'), 263),\n",
       "             (('VBP', 'NNP'), 264),\n",
       "             (('VBN', 'VBG'), 265),\n",
       "             (('NNP', 'JJS'), 266),\n",
       "             (('JJS', 'RB'), 267),\n",
       "             (('IN', 'WDT'), 268),\n",
       "             (('NN', 'MD'), 269),\n",
       "             (('VBN', '$'), 270),\n",
       "             (('NN', '$'), 271),\n",
       "             (('$', 'DT'), 272),\n",
       "             (('$', 'VBZ'), 273),\n",
       "             (('IN', ':'), 274),\n",
       "             (('VBD', 'VB'), 275),\n",
       "             (('VB', 'JJ'), 276),\n",
       "             (('CC', 'JJ'), 277),\n",
       "             (('VBN', 'NNP'), 278),\n",
       "             (('VBZ', '``'), 279),\n",
       "             (('VBZ', ':'), 280),\n",
       "             (('CD', 'RB'), 281),\n",
       "             (('VBN', 'RBR'), 282),\n",
       "             (('MD', 'CC'), 283),\n",
       "             (('NNP', 'NNPS'), 284),\n",
       "             (('NNP', '``'), 285),\n",
       "             (('NNP', \"''\"), 286),\n",
       "             (('JJR', '$'), 287),\n",
       "             (('JJS', 'VBN'), 288),\n",
       "             (('$', 'TO'), 289),\n",
       "             (('$', '$'), 290),\n",
       "             (('VBG', 'VBG'), 291),\n",
       "             (('VBN', 'VBN'), 292),\n",
       "             (('MD', 'VBG'), 293),\n",
       "             (('VBG', 'RP'), 294),\n",
       "             (('NNS', 'WP$'), 295),\n",
       "             (('VB', '``'), 296),\n",
       "             (('RB', 'NNS'), 297),\n",
       "             (('CD', 'CC'), 298),\n",
       "             (('CC', 'MD'), 299),\n",
       "             (('JJ', '$'), 300),\n",
       "             (('VBG', '$'), 301),\n",
       "             (('NNPS', 'DT'), 302),\n",
       "             (('NNPS', 'NNP'), 303),\n",
       "             (('NNPS', 'POS'), 304),\n",
       "             (('VBD', 'RBR'), 305),\n",
       "             (('RBR', 'IN'), 306),\n",
       "             (('VB', 'VB'), 307),\n",
       "             (('VBZ', 'VBD'), 308),\n",
       "             (('IN', 'CC'), 309),\n",
       "             (('DT', 'JJS'), 310),\n",
       "             (('VBP', 'EX'), 311),\n",
       "             (('VBN', 'VBD'), 312),\n",
       "             (('VBN', 'JJ'), 313),\n",
       "             (('VBG', 'NNP'), 314),\n",
       "             (('NN', '``'), 315),\n",
       "             (('NN', \"''\"), 316),\n",
       "             (('VBP', 'MD'), 317),\n",
       "             (('VBP', \"''\"), 318),\n",
       "             (('VBG', 'JJ'), 319),\n",
       "             (('JJ', 'MD'), 320),\n",
       "             (('CD', 'JJR'), 321),\n",
       "             (('DT', 'CD'), 322),\n",
       "             (('TO', 'IN'), 323),\n",
       "             (('DT', 'VBN'), 324),\n",
       "             (('VBP', 'VBD'), 325),\n",
       "             (('$', 'VBD'), 326),\n",
       "             (('MD', 'WP'), 327),\n",
       "             (('NNP', 'MD'), 328),\n",
       "             (('IN', 'NNPS'), 329),\n",
       "             (('VBP', 'VB'), 330),\n",
       "             (('VBN', '``'), 331),\n",
       "             (('DT', 'VBZ'), 332),\n",
       "             (('NN', '('), 333),\n",
       "             (('NN', ')'), 334),\n",
       "             (('VB', 'VBG'), 335),\n",
       "             (('VBG', 'PRP'), 336),\n",
       "             (('TO', 'CC'), 337),\n",
       "             (('CC', 'TO'), 338),\n",
       "             (('RB', 'TO'), 339),\n",
       "             (('VB', 'CD'), 340),\n",
       "             (('JJ', ','), 341),\n",
       "             (('JJS', ':'), 342),\n",
       "             (('NN', 'PDT'), 343),\n",
       "             (('MD', '('), 344),\n",
       "             (('MD', ')'), 345),\n",
       "             (('$', 'JJ'), 346),\n",
       "             (('VBD', 'EX'), 347),\n",
       "             (('NNP', 'NNS'), 348),\n",
       "             (('VBP', '$'), 349),\n",
       "             (('NNPS', 'CC'), 350),\n",
       "             (('NNS', 'VB'), 351),\n",
       "             (('CC', 'NNPS'), 352),\n",
       "             (('MD', 'WDT'), 353),\n",
       "             (('NNS', 'MD'), 354),\n",
       "             (('JJ', 'RBR'), 355),\n",
       "             (('$', '('), 356),\n",
       "             (('NNS', '$'), 357),\n",
       "             (('$', ')'), 358),\n",
       "             (('PRP', 'DT'), 359),\n",
       "             (('PRP', 'CC'), 360),\n",
       "             (('VBN', 'RP'), 361),\n",
       "             (('CD', ':'), 362),\n",
       "             (('CD', 'VBG'), 363),\n",
       "             (('VBP', ':'), 364),\n",
       "             (('VBG', 'VBN'), 365),\n",
       "             (('RB', 'VBN'), 366),\n",
       "             (('*', 'VB'), 367),\n",
       "             (('VB', '.'), 368),\n",
       "             (('RB', 'JJR'), 369),\n",
       "             (('NNP', 'WRB'), 370),\n",
       "             (('NNP', 'RP'), 371),\n",
       "             (('VBP', 'RBS'), 372),\n",
       "             (('RBS', 'IN'), 373),\n",
       "             (('NN', 'VBP'), 374),\n",
       "             (('MD', 'VBN'), 375),\n",
       "             (('VBG', \"''\"), 376),\n",
       "             (('RB', 'VBZ'), 377),\n",
       "             (('VBZ', 'RP'), 378),\n",
       "             (('RB', 'CC'), 379),\n",
       "             (('CC', 'RB'), 380),\n",
       "             (('VB', 'RBR'), 381),\n",
       "             (('CD', 'POS'), 382),\n",
       "             (('MD', 'DT'), 383),\n",
       "             (('DT', 'VBG'), 384),\n",
       "             (('TO', 'VBG'), 385),\n",
       "             (('MD', 'EX'), 386),\n",
       "             (('VBZ', 'MD'), 387),\n",
       "             (('NNPS', 'JJ'), 388),\n",
       "             (('VBP', 'NNPS'), 389),\n",
       "             (('VBP', 'VBP'), 390),\n",
       "             (('RP', 'VBG'), 391),\n",
       "             (('RB', ','), 392),\n",
       "             (('NNP', 'RB'), 393),\n",
       "             (('$', 'VBN'), 394),\n",
       "             (('NN', 'WP'), 395),\n",
       "             (('VBG', ':'), 396),\n",
       "             (('RB', 'VBG'), 397),\n",
       "             (('CC', ','), 398),\n",
       "             (('IN', 'RBR'), 399),\n",
       "             (('VBZ', 'JJR'), 400),\n",
       "             (('VB', 'VBP'), 401),\n",
       "             (('RB', 'DT'), 402),\n",
       "             (('NNP', '('), 403),\n",
       "             (('NNP', ')'), 404),\n",
       "             (('RB', 'JJ'), 405),\n",
       "             (('VBP', 'JJS'), 406),\n",
       "             (('RB', ':'), 407),\n",
       "             (('RB', 'WP'), 408),\n",
       "             (('VBZ', '('), 409),\n",
       "             (('VBZ', ')'), 410),\n",
       "             (('NNP', 'PRP'), 411),\n",
       "             (('VBZ', 'FW'), 412),\n",
       "             ((':', 'VBN'), 413),\n",
       "             ((':', ':'), 414),\n",
       "             (('VB', 'MD'), 415),\n",
       "             (('IN', 'WRB'), 416),\n",
       "             (('JJ', 'PRP'), 417),\n",
       "             (('JJ', 'RBS'), 418),\n",
       "             (('VBP', 'DT'), 419),\n",
       "             (('VBG', 'VB'), 420),\n",
       "             (('VB', 'VBD'), 421),\n",
       "             (('JJR', 'CC'), 422),\n",
       "             (('CC', 'JJR'), 423),\n",
       "             (('RBR', 'NN'), 424),\n",
       "             (('RB', 'VBD'), 425),\n",
       "             (('CD', 'VBN'), 426),\n",
       "             (('JJ', 'NNP'), 427),\n",
       "             (('TO', 'DT'), 428),\n",
       "             (('RBR', 'NNS'), 429),\n",
       "             (('VBZ', 'RBR'), 430),\n",
       "             (('IN', '('), 431),\n",
       "             (('RBR', 'RB'), 432),\n",
       "             (('RBR', 'DT'), 433),\n",
       "             (('RBR', 'VBN'), 434),\n",
       "             (('IN', ')'), 435),\n",
       "             (('RB', 'WRB'), 436),\n",
       "             (('RB', 'VBP'), 437),\n",
       "             (('NNS', 'PRP'), 438),\n",
       "             (('PRP', 'VBD'), 439),\n",
       "             (('NN', 'WP$'), 440),\n",
       "             (('CD', 'VBZ'), 441),\n",
       "             (('(', 'VBN'), 442),\n",
       "             (('(', ')'), 443),\n",
       "             (('VBZ', 'JJS'), 444),\n",
       "             (('*', 'NNP'), 445),\n",
       "             (('VBG', '``'), 446),\n",
       "             (('NNP', 'VB'), 447),\n",
       "             (('JJS', 'PRP$'), 448),\n",
       "             (('JJR', 'DT'), 449),\n",
       "             (('JJR', 'CD'), 450),\n",
       "             (('TO', '``'), 451),\n",
       "             (('*', 'TO'), 452),\n",
       "             (('TO', ','), 453),\n",
       "             (('TO', \"''\"), 454),\n",
       "             (('TO', 'VBZ'), 455),\n",
       "             (('TO', '.'), 456),\n",
       "             (('VB', ':'), 457),\n",
       "             (('*', 'NNS'), 458),\n",
       "             (('NNS', '.'), 459),\n",
       "             (('JJ', 'PRP$'), 460),\n",
       "             (('NN', 'FW'), 461),\n",
       "             (('JJ', 'VBD'), 462),\n",
       "             (('NN', 'PRP'), 463),\n",
       "             (('RB', '$'), 464),\n",
       "             (('CD', 'NNP'), 465),\n",
       "             (('VBN', 'CD'), 466),\n",
       "             (('MD', 'WRB'), 467),\n",
       "             (('MD', 'TO'), 468),\n",
       "             (('MD', \"''\"), 469),\n",
       "             (('IN', 'VB'), 470),\n",
       "             (('VBN', 'VBZ'), 471),\n",
       "             (('VBD', 'UH'), 472),\n",
       "             (('NNPS', 'IN'), 473),\n",
       "             (('NNPS', ','), 474),\n",
       "             (('NNPS', 'NN'), 475),\n",
       "             (('VBG', 'NNPS'), 476),\n",
       "             (('VBZ', 'NNPS'), 477),\n",
       "             (('JJS', 'NNP'), 478),\n",
       "             (('JJS', 'CC'), 479),\n",
       "             (('CC', 'JJS'), 480),\n",
       "             (('JJ', 'VBG'), 481),\n",
       "             (('CC', 'CC'), 482),\n",
       "             (('PRP', 'RB'), 483),\n",
       "             (('NN', 'RBS'), 484),\n",
       "             (('PRP', ','), 485),\n",
       "             (('PRP', 'VBZ'), 486),\n",
       "             (('VB', 'VBZ'), 487),\n",
       "             (('RP', 'TO'), 488),\n",
       "             (('VBG', 'VBD'), 489),\n",
       "             (('RB', '``'), 490),\n",
       "             (('*', 'RB'), 491),\n",
       "             (('RB', '.'), 492),\n",
       "             (('TO', ':'), 493),\n",
       "             (('TO', 'JJ'), 494),\n",
       "             (('DT', 'PDT'), 495),\n",
       "             (('NNS', 'PDT'), 496),\n",
       "             (('VBN', 'PRP'), 497),\n",
       "             (('VBN', 'NNPS'), 498),\n",
       "             (('VBD', '('), 499),\n",
       "             (('VBD', ')'), 500),\n",
       "             (('NN', 'NNPS'), 501),\n",
       "             (('RB', \"''\"), 502),\n",
       "             (('NNS', 'NNPS'), 503),\n",
       "             (('VBN', 'WRB'), 504),\n",
       "             (('VBN', 'WDT'), 505),\n",
       "             (('VBD', 'NNPS'), 506),\n",
       "             (('VBN', 'MD'), 507),\n",
       "             (('*', 'VBN'), 508),\n",
       "             (('VBN', '.'), 509),\n",
       "             (('$', 'JJR'), 510),\n",
       "             (('CD', 'JJS'), 511),\n",
       "             (('CD', 'TO'), 512),\n",
       "             (('CD', 'VBD'), 513),\n",
       "             (('VBG', 'MD'), 514),\n",
       "             (('NN', 'RBR'), 515),\n",
       "             (('*', 'VBG'), 516),\n",
       "             (('NNP', 'SYM'), 517),\n",
       "             (('DT', 'JJR'), 518),\n",
       "             (('VBP', 'RBR'), 519),\n",
       "             (('*', 'IN'), 520),\n",
       "             (('NN', 'LS'), 521),\n",
       "             (('LS', ':'), 522),\n",
       "             (('JJ', 'JJR'), 523),\n",
       "             (('CC', '``'), 524),\n",
       "             (('JJR', 'JJ'), 525),\n",
       "             (('VBN', 'VB'), 526),\n",
       "             (('VBD', 'JJS'), 527),\n",
       "             (('NN', 'VB'), 528),\n",
       "             (('NNP', 'VBG'), 529),\n",
       "             (('RB', 'JJS'), 530),\n",
       "             (('*', 'NNPS'), 531),\n",
       "             (('NNPS', ':'), 532),\n",
       "             (('NNPS', '.'), 533),\n",
       "             (('$', 'RBR'), 534),\n",
       "             (('MD', 'NNPS'), 535),\n",
       "             (('MD', '$'), 536),\n",
       "             (('WRB', 'JJ'), 537),\n",
       "             (('DT', 'MD'), 538),\n",
       "             (('NNP', 'VBP'), 539),\n",
       "             (('JJ', 'VBP'), 540),\n",
       "             (('TO', 'RBR'), 541),\n",
       "             (('(', 'CC'), 542),\n",
       "             (('NNPS', 'NNS'), 543),\n",
       "             (('JJS', ','), 544),\n",
       "             (('JJS', 'VBZ'), 545),\n",
       "             (('NNP', 'TO'), 546),\n",
       "             (('NNP', '$'), 547),\n",
       "             (('VBZ', 'LS'), 548),\n",
       "             (('LS', ')'), 549),\n",
       "             (('NNS', '('), 550),\n",
       "             (('NNS', ')'), 551),\n",
       "             (('MD', 'MD'), 552),\n",
       "             (('CC', ':'), 553),\n",
       "             (('MD', 'LS'), 554),\n",
       "             (('JJ', 'RP'), 555),\n",
       "             (('CD', '('), 556),\n",
       "             (('CD', ')'), 557),\n",
       "             (('IN', '.'), 558),\n",
       "             (('TO', 'NNPS'), 559),\n",
       "             (('RB', 'NNP'), 560),\n",
       "             (('JJ', 'WP'), 561),\n",
       "             (('JJ', 'POS'), 562),\n",
       "             (('VBG', 'RBR'), 563),\n",
       "             (('RBR', 'TO'), 564),\n",
       "             (('RBR', 'CC'), 565),\n",
       "             (('CC', 'RBR'), 566),\n",
       "             (('VBN', \"''\"), 567),\n",
       "             (('CD', 'RP'), 568),\n",
       "             (('$', ':'), 569),\n",
       "             (('*', '$'), 570),\n",
       "             (('$', '.'), 571),\n",
       "             (('CC', 'DT'), 572),\n",
       "             (('$', 'VBG'), 573),\n",
       "             (('RBR', 'JJR'), 574),\n",
       "             (('JJS', '``'), 575),\n",
       "             (('JJS', 'MD'), 576),\n",
       "             (('NN', 'RP'), 577),\n",
       "             (('RB', 'RBS'), 578),\n",
       "             (('WP', 'RB'), 579),\n",
       "             (('WRB', 'RB'), 580),\n",
       "             (('CD', 'MD'), 581),\n",
       "             (('DT', ':'), 582),\n",
       "             (('VBG', 'DT'), 583),\n",
       "             (('DT', 'VBD'), 584),\n",
       "             (('MD', 'CD'), 585),\n",
       "             (('JJ', 'VBN'), 586),\n",
       "             (('TO', 'TO'), 587),\n",
       "             (('DT', 'JJ'), 588),\n",
       "             (('VB', 'LS'), 589),\n",
       "             (('LS', '.'), 590),\n",
       "             ((':', 'VBG'), 591),\n",
       "             (('$', 'PRP$'), 592),\n",
       "             (('RB', 'VB'), 593),\n",
       "             (('$', 'VBP'), 594),\n",
       "             (('CD', '.'), 595),\n",
       "             (('JJ', '('), 596),\n",
       "             (('JJ', ')'), 597),\n",
       "             (('JJ', '``'), 598),\n",
       "             (('JJ', \"''\"), 599),\n",
       "             (('VBN', 'DT'), 600),\n",
       "             (('TO', 'JJS'), 601),\n",
       "             ((',', 'NNS'), 602),\n",
       "             ((',', 'VBN'), 603),\n",
       "             (('VBP', '('), 604),\n",
       "             (('VBP', ')'), 605),\n",
       "             (('IN', 'WP'), 606),\n",
       "             (('MD', 'VBP'), 607),\n",
       "             (('JJ', 'JJS'), 608),\n",
       "             (('PRP', 'VBP'), 609),\n",
       "             (('JJ', 'WRB'), 610),\n",
       "             (('MD', 'JJ'), 611),\n",
       "             (('IN', 'PDT'), 612),\n",
       "             (('NNPS', 'JJS'), 613),\n",
       "             (('VB', 'NNPS'), 614),\n",
       "             (('NNPS', 'VBP'), 615),\n",
       "             (('JJR', 'NNS'), 616),\n",
       "             (('IN', 'PRP$'), 617),\n",
       "             (('PRP', 'NNS'), 618),\n",
       "             (('NNPS', 'NNPS'), 619),\n",
       "             (('#', 'IN'), 620),\n",
       "             (('VBZ', '#'), 621),\n",
       "             (('#', 'CD'), 622),\n",
       "             (('#', '$'), 623),\n",
       "             (('$', 'NNP'), 624),\n",
       "             (('JJ', 'VBZ'), 625),\n",
       "             (('DT', 'DT'), 626),\n",
       "             (('$', 'JJS'), 627),\n",
       "             (('TO', 'WRB'), 628),\n",
       "             (('.', 'NNP'), 629),\n",
       "             (('*', '.'), 630),\n",
       "             (('IN', 'FW'), 631),\n",
       "             (('VBZ', 'UH'), 632),\n",
       "             (('TO', 'VBP'), 633),\n",
       "             (('NNS', 'WDT'), 634),\n",
       "             (('MD', 'JJS'), 635),\n",
       "             (('JJR', 'PRP'), 636),\n",
       "             (('PRP', 'IN'), 637),\n",
       "             (('*', 'WP'), 638),\n",
       "             (('WP', 'IN'), 639),\n",
       "             (('WP', '.'), 640),\n",
       "             (('RBR', 'JJ'), 641),\n",
       "             (('*', 'JJ'), 642),\n",
       "             (('JJ', '.'), 643),\n",
       "             (('DT', 'CC'), 644),\n",
       "             (('CC', 'PRP'), 645),\n",
       "             (('VBG', '('), 646),\n",
       "             (('VBG', ')'), 647),\n",
       "             (('VBN', 'JJR'), 648),\n",
       "             (('DT', 'TO'), 649),\n",
       "             (('VB', ')'), 650),\n",
       "             (('IN', '#'), 651),\n",
       "             (('#', '('), 652),\n",
       "             (('#', ')'), 653),\n",
       "             (('VBN', 'RBS'), 654),\n",
       "             (('CD', \"''\"), 655),\n",
       "             (('NN', 'WDT'), 656),\n",
       "             (('CD', 'PRP'), 657),\n",
       "             (('VBP', 'UH'), 658),\n",
       "             (('CD', 'PRP$'), 659),\n",
       "             (('VBG', 'JJR'), 660),\n",
       "             (('DT', ','), 661),\n",
       "             (('DT', 'NNS'), 662),\n",
       "             (('TO', 'JJR'), 663),\n",
       "             (('NNS', 'RBS'), 664),\n",
       "             (('VBG', '.'), 665),\n",
       "             (('VBZ', 'PRP$'), 666),\n",
       "             (('JJR', 'TO'), 667),\n",
       "             (('NNPS', 'RB'), 668),\n",
       "             (('NNPS', 'VBN'), 669),\n",
       "             (('$', \"''\"), 670),\n",
       "             (('NNP', 'PDT'), 671),\n",
       "             (('$', 'NNS'), 672),\n",
       "             (('NNPS', 'VBZ'), 673),\n",
       "             (('TO', '#'), 674),\n",
       "             (('#', ','), 675),\n",
       "             (('#', 'CC'), 676),\n",
       "             (('VBD', '#'), 677),\n",
       "             (('#', 'TO'), 678),\n",
       "             (('#', '#'), 679),\n",
       "             (('VBN', 'JJS'), 680),\n",
       "             (('TO', 'WP'), 681),\n",
       "             (('JJR', 'VB'), 682),\n",
       "             (('VBG', 'PRP$'), 683),\n",
       "             (('``', 'RB'), 684),\n",
       "             (('``', 'JJ'), 685),\n",
       "             (('``', \"''\"), 686),\n",
       "             (('``', 'NNS'), 687),\n",
       "             (('NNPS', 'VBD'), 688),\n",
       "             ((':', 'CC'), 689),\n",
       "             (('JJR', '``'), 690),\n",
       "             (('JJR', \"''\"), 691),\n",
       "             (('RB', 'MD'), 692),\n",
       "             (('VBD', 'RBS'), 693),\n",
       "             (('RBS', 'JJ'), 694),\n",
       "             (('NNS', 'WRB'), 695),\n",
       "             ((',', 'VB'), 696),\n",
       "             (('CC', ')'), 697),\n",
       "             (('DT', '``'), 698),\n",
       "             (('JJS', 'JJ'), 699),\n",
       "             (('TO', 'MD'), 700),\n",
       "             (('(', 'VBG'), 701),\n",
       "             (('RBS', 'DT'), 702),\n",
       "             (('VBZ', 'RBS'), 703),\n",
       "             (('RBS', 'VBN'), 704),\n",
       "             (('RB', 'PDT'), 705),\n",
       "             (('VB', 'RBS'), 706),\n",
       "             (('TO', 'WDT'), 707),\n",
       "             (('NNS', 'FW'), 708),\n",
       "             (('PRP$', 'CC'), 709),\n",
       "             (('CC', 'PRP$'), 710),\n",
       "             (('MD', 'FW'), 711),\n",
       "             (('JJS', 'JJR'), 712),\n",
       "             (('NNP', 'FW'), 713),\n",
       "             (('PRP', ':'), 714),\n",
       "             (('#', 'JJR'), 715),\n",
       "             (('VBP', 'LS'), 716),\n",
       "             (('FW', 'DT'), 717),\n",
       "             (('VB', '('), 718),\n",
       "             (('FW', 'FW'), 719),\n",
       "             (('(', 'VB'), 720),\n",
       "             (('NNPS', 'JJR'), 721),\n",
       "             (('DT', '('), 722),\n",
       "             (('DT', ')'), 723),\n",
       "             (('RB', '('), 724),\n",
       "             (('RB', ')'), 725),\n",
       "             (('FW', 'PRP$'), 726),\n",
       "             (('VBD', 'FW'), 727),\n",
       "             (('FW', 'IN'), 728),\n",
       "             (('WRB', '``'), 729),\n",
       "             (('WRB', 'FW'), 730),\n",
       "             (('WRB', \"''\"), 731),\n",
       "             (('FW', 'RB'), 732),\n",
       "             (('VBG', 'FW'), 733),\n",
       "             (('JJ', 'UH'), 734),\n",
       "             (('RB', 'PRP'), 735),\n",
       "             (('DT', 'VB'), 736),\n",
       "             (('PRP', 'NNP'), 737),\n",
       "             (('NNPS', 'CD'), 738),\n",
       "             (('JJR', 'VBN'), 739),\n",
       "             (('VB', 'PRP$'), 740),\n",
       "             (('VB', 'WDT'), 741),\n",
       "             (('CD', 'NNS'), 742),\n",
       "             (('MD', 'JJR'), 743),\n",
       "             (('``', 'PRP'), 744),\n",
       "             (('CD', '``'), 745),\n",
       "             (('IN', 'RBS'), 746),\n",
       "             (('PRP', 'POS'), 747),\n",
       "             (('(', 'MD'), 748)])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_cp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DepSample(idx=0, token='*', pos='*', head=0),\n",
       " DepSample(idx=1, token='What', pos='WP', head=2),\n",
       " DepSample(idx=2, token=\"'s\", pos='VBZ', head=0),\n",
       " DepSample(idx=3, token='next', pos='JJ', head=2),\n",
       " DepSample(idx=4, token='?', pos='.', head=2)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{432: 3, 433: 3, 27: 3}\n",
      "{2079: 1, 2080: 1, 318: 1, 730: 1, 246: 1, 47: 1, 1301: 1, 1302: 1, 15: 1, 4766: 1, 4767: 1, 42: 1}\n",
      "OrderedDict([(27, 3), (432, 3), (433, 3), (18911, 1), (18938, 1), (18943, 1), (19142, 1), (19214, 1), (19626, 1), (20197, 1), (20198, 1), (20975, 1), (20976, 1), (23662, 1), (23663, 1)])\n"
     ]
    }
   ],
   "source": [
    "hw_hp_ind = extract_hw_hp_feat_indices(s, unigram_feat_dict[0])\n",
    "cw_cp_ind = extract_cw_cp_feat_indices(s, unigram_feat_dict[1])\n",
    "unigrams_ind = extract_unigram_feat_indices(s, unigram_feat_dict)\n",
    "print(hw_hp_ind)\n",
    "print(cw_cp_ind)\n",
    "print(unigrams_ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
