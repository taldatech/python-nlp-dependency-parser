{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "import copy\n",
    "# named tuple has methods like _asdict()\n",
    "ROOT = \"*\"\n",
    "DepSample = namedtuple('DepSample', 'idx, token, pos, head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"1\tPierre\t_\tNNP\t_\t_\t2\tNAME\t_\t_\"\n",
    "example = example.split('\\t')\n",
    "example = DepSample(example[0], example[1], example[3], example[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_sample_generator(path_to_file):\n",
    "    \"\"\"\n",
    "    This function generates samples, such that every sample is a list\n",
    "    ordered by the tokens' counter (first column).\n",
    "    :param: path_to_file: string to the location of the file tor read from (str)\n",
    "    :return: sample (list of DepSample)\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(path_to_file), \"File does not exist\"\n",
    "    root = DepSample(0, ROOT, ROOT, 0)\n",
    "    with open(path_to_file) as fp:\n",
    "        sample = [root]\n",
    "        for line in fp:\n",
    "            if not line.rstrip():\n",
    "                yield sample\n",
    "                sample = [root]\n",
    "            else:\n",
    "                ls = line.rstrip().split('\\t')\n",
    "#                 print(ls)\n",
    "                try:\n",
    "                    head = int(ls[6])\n",
    "                except ValueError:\n",
    "                    head = ls[6]\n",
    "                sample.append(DepSample(int(ls[0]), ls[1], ls[3], head))\n",
    "        if len(sample) > 1:\n",
    "            yield sample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = './data/train.labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validation(path_to_file, valid_amount=0.2):\n",
    "    \"\"\"\n",
    "    This functions takes a train dataset and splits it to trainining\n",
    "    and validation sets accoording to `valid_amount`.\n",
    "    :param: path_to_file: path to file containing the dataset (str)\n",
    "    :param: valid_amount: percentage of samples to take for validation (float)\n",
    "    :return: train_file_path: path to file containing the training samples (str)\n",
    "    :return: valid_file_path: path to file containing the validation samples (str)\n",
    "    \"\"\"\n",
    "    path_train_file = path_to_file + \".train.labeled\"\n",
    "    path_valid_file = path_to_file + \".valid.labeled\"\n",
    "    # count samples\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    total_samples = 0\n",
    "    for _ in samp_gen:\n",
    "        total_samples += 1\n",
    "    print(\"total samples \", total_samples)\n",
    "    buffer = []\n",
    "    num_validation = int(valid_amount * total_samples)\n",
    "    num_training = total_samples - num_validation\n",
    "    taken_for_training = 0\n",
    "    t_file = open(path_train_file, 'w')\n",
    "    v_file = open(path_valid_file, 'w')\n",
    "    with open(path_to_file) as fp:\n",
    "        sample = []\n",
    "        for line in fp:\n",
    "            if not line.rstrip():\n",
    "                if taken_for_training < num_training:\n",
    "                    for l in sample:\n",
    "                        t_file.write(l)\n",
    "                    t_file.write('\\n')\n",
    "                    taken_for_training += 1\n",
    "                else:\n",
    "                    for l in sample:\n",
    "                        v_file.write(l)\n",
    "                    v_file.write('\\n')\n",
    "                sample = []\n",
    "            else:\n",
    "                sample.append(line)\n",
    "                \n",
    "        if taken_for_training < num_training:\n",
    "            for l in sample:\n",
    "                t_file.write(l)\n",
    "            t_file.write('\\n')\n",
    "            taken_for_training += 1\n",
    "        else:\n",
    "            for l in sample:\n",
    "                v_file.write(l)\n",
    "            v_file.write('\\n')\n",
    "    t_file.close()\n",
    "    v_file.close()\n",
    "    print(\"num training: \", num_training, \" saved @ \", path_train_file)\n",
    "    print(\"num validation: \", num_validation, \" saved @ \", path_valid_file)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples  5000\n",
      "num training:  4000  saved @  ./data/train.labeled.train.labeled\n",
      "num validation:  1000  saved @  ./data/train.labeled.valid.labeled\n"
     ]
    }
   ],
   "source": [
    "split_train_validation(path_to_file, valid_amount=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_file = './data/train.labeled.train.labeled'\n",
    "path_to_Valid_file = './data/train.labeled.valid.labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DepSample(idx=0, token='*', pos='*', head=0), DepSample(idx=1, token='Alan', pos='NNP', head=2), DepSample(idx=2, token='Spoon', pos='NNP', head=9), DepSample(idx=3, token=',', pos=',', head=2), DepSample(idx=4, token='recently', pos='RB', head=5), DepSample(idx=5, token='named', pos='VBN', head=2), DepSample(idx=6, token='Newsweek', pos='NNP', head=7), DepSample(idx=7, token='president', pos='NN', head=5), DepSample(idx=8, token=',', pos=',', head=2), DepSample(idx=9, token='said', pos='VBD', head=0), DepSample(idx=10, token='Newsweek', pos='NNP', head=13), DepSample(idx=11, token=\"'s\", pos='POS', head=10), DepSample(idx=12, token='ad', pos='NN', head=13), DepSample(idx=13, token='rates', pos='NNS', head=14), DepSample(idx=14, token='would', pos='MD', head=9), DepSample(idx=15, token='increase', pos='VB', head=14), DepSample(idx=16, token='5', pos='CD', head=17), DepSample(idx=17, token='%', pos='NN', head=15), DepSample(idx=18, token='in', pos='IN', head=15), DepSample(idx=19, token='January', pos='NNP', head=18), DepSample(idx=20, token='.', pos='.', head=9)]\n"
     ]
    }
   ],
   "source": [
    "samp_gen = dep_sample_generator(path_to_file=path_to_train_file)\n",
    "for s_i, s in enumerate(samp_gen):\n",
    "    if s_i == 100:\n",
    "        print(s)\n",
    "#     if s_i > 0:\n",
    "#         break\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Sample\n",
    "\n",
    "number of tokens in a sample = `sample[-1].idx`\n",
    "\n",
    "POS tags = `[s.pos for s in sample]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample length:  4\n",
      "POS tags:  ['*', 'WP', 'VBZ', 'JJ', '.']\n",
      "Heads:  [0, 2, 0, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DepSample(idx=0, token='*', pos='*', head=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_len = s[-1].idx\n",
    "s_tags = [l.pos for l in s]\n",
    "s_heads = [l.head for l in s]\n",
    "print(\"sample length: \", sample_len)\n",
    "print(\"POS tags: \", s_tags)\n",
    "print(\"Heads: \", s_heads)\n",
    "s[s_heads[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "### Unigram\n",
    "* (head_word, head_pos)\n",
    "* (head_word)\n",
    "* (head_pos)\n",
    "* (child_word, child_pos)\n",
    "* (child_word)\n",
    "* (child_pos)\n",
    "\n",
    "### Bigram\n",
    "* (head_word, head_pos, child_word, child_pos)\n",
    "* (head_pos, child_word, child_pos)\n",
    "* (head_word, child_word, child_pos)\n",
    "* (head_word, head_pos, child_pos)\n",
    "* (head_word, head_pos, child_word)\n",
    "* (head_word, child_word)\n",
    "* (head_pos, child_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_hist_dict(path_to_file, save_to_file=False):\n",
    "    \"\"\"\n",
    "    This function generates histogram of of the tokens in the dataset.\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: save_to_file: whether or not to save the dictionary on disk (bool)\n",
    "    :return: word_hist: OrderedDict word->word_count\n",
    "    \"\"\"\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    word_hist = {}\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT:\n",
    "                continue\n",
    "            if word_hist.get(s.token):\n",
    "                word_hist[s.token] += 1\n",
    "            else:\n",
    "                word_hist[s.token] = 1\n",
    "    word_hist = OrderedDict(sorted(word_hist.items(), key=lambda t: -t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".word.hist\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(word_hist, fp)\n",
    "        print(\"word histogram dictionary saved @ \", path)\n",
    "    return word_hist\n",
    "\n",
    "\"\"\"\n",
    "UNIGRAMS\n",
    "\"\"\"\n",
    "\n",
    "def generate_hw_hp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            # ignore ROOT\n",
    "            if s.token == ROOT or (word_hist.get(sample[s.head].token) \\\n",
    "                                   and word_hist[sample[s.head].token] < word_threshold):\n",
    "                continue\n",
    "            if sample[s.head].token == ROOT:\n",
    "                continue\n",
    "            feats = [(sample[s.head].token, sample[s.head].pos),\n",
    "                     (sample[s.head].token),\n",
    "                     (sample[s.head].pos)]\n",
    "            for feat in feats:\n",
    "                if hw_hp_feat_dict.get(feat) is None:\n",
    "                    hw_hp_feat_dict[feat] = current_idx\n",
    "                    current_idx += 1\n",
    "    print(\"total (head_word, head_pos), (head_word), (head_pos) features: \", current_idx)\n",
    "    hw_hp_feat_dict = OrderedDict(sorted(hw_hp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos), (head_word), (head_pos) features dictionary @ \", path)\n",
    "    return hw_hp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_hp_feat_indices(sample, hw_hp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_hp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_hp_dict.get((sample[s.head].token, sample[s.head].pos)):\n",
    "            idx = hw_hp_dict.get((sample[s.head].token, sample[s.head].pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if hw_hp_dict.get((sample[s.head].token)):\n",
    "            idx = hw_hp_dict.get((sample[s.head].token))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if hw_hp_dict.get((sample[s.head].pos)):\n",
    "            idx = hw_hp_dict.get((sample[s.head].pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hw_hp_feat_indices_pair(head, child, hw_hp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_hp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_list: list of indices\n",
    "    \"\"\"\n",
    "    feat_indices = []\n",
    "    if hw_hp_dict.get((head.token, head.pos)):\n",
    "        feat_indices.append(hw_hp_dict.get((head.token, head.pos)))\n",
    "    if hw_hp_dict.get(head.token):\n",
    "        feat_indices.append(hw_hp_dict.get(head.token))\n",
    "    if hw_hp_dict.get(head.pos):\n",
    "        feat_indices.append(hw_hp_dict.get(head.pos))\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def generate_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feats = [(s.token, s.pos),\n",
    "                     (s.token),\n",
    "                     (s.pos)]\n",
    "            for feat in feats:\n",
    "                if cw_cp_feat_dict.get(feat) is None:\n",
    "                    cw_cp_feat_dict[feat] = current_idx\n",
    "                    current_idx += 1\n",
    "    print(\"total (child_word, child_pos), (child_word), (child_pos) features: \", current_idx)\n",
    "    cw_cp_feat_dict = OrderedDict(sorted(cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(cw_cp_feat_dict, fp)\n",
    "        print(\"saved (child_word, child_pos), (child_word), (child_pos) features dictionary @ \", path)\n",
    "    return cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_cw_cp_feat_indices(sample, cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if cw_cp_dict.get((s.token, s.pos)):\n",
    "            idx = cw_cp_dict.get((s.token, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if cw_cp_dict.get((s.token)):\n",
    "            idx = cw_cp_dict.get((s.token))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "        if cw_cp_dict.get((s.pos)):\n",
    "            idx = cw_cp_dict.get((s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_cw_cp_feat_indices_pair(head, child, cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_list: list of indices\n",
    "    \"\"\"\n",
    "    feat_indices = []\n",
    "    if cw_cp_dict.get((child.token, child.pos)):\n",
    "        feat_indices.append(cw_cp_dict.get((child.token, child.pos)))\n",
    "    if cw_cp_dict.get(child.token):\n",
    "        feat_indices.append(cw_cp_dict.get(child.token))\n",
    "    if cw_cp_dict.get(child.pos):\n",
    "        feat_indices.append(cw_cp_dict.get(child.pos))\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def generate_unigram_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: unigram_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    hw_hp_dict = generate_hw_hp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                          save_to_file=save_to_file, word_hist=word_hist)\n",
    "    cw_cp_dict = generate_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                          save_to_file=save_to_file, word_hist=word_hist)\n",
    "    print(\"total unigrams features: \", len(hw_hp_dict) + len(cw_cp_dict))\n",
    "    return hw_hp_dict, cw_cp_dict\n",
    "\n",
    "\n",
    "def extract_unigram_feat_indices_pair(head, child, unigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: unigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_list: list of indices\n",
    "    \"\"\"\n",
    "    num_hw_hp_feats = len(unigram_dict[0])\n",
    "    num_cw_cp_feats = len(unigram_dict[1])\n",
    "    current_num_features = 0\n",
    "    hw_hp_ind = extract_hw_hp_feat_indices_pair(head, child, unigram_dict[0])\n",
    "    current_num_features += num_hw_hp_feats\n",
    "    cw_cp_ind = extract_cw_cp_feat_indices_pair(head, child, unigram_dict[1])\n",
    "    unigram_indices = copy.deepcopy(hw_hp_ind)\n",
    "    for i in cw_cp_ind:\n",
    "        unigram_indices.append(current_num_features + i)\n",
    "    current_num_features += num_cw_cp_feats\n",
    "    return sorted(unigram_indices)\n",
    "\n",
    "\n",
    "def extract_unigram_feat_indices(sample, unigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the unigrams features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: unigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    num_hw_hp_feats = len(unigram_dict[0])\n",
    "    num_cw_cp_feats = len(unigram_dict[1])\n",
    "    current_num_features = 0\n",
    "    hw_hp_ind = extract_hw_hp_feat_indices(sample, unigram_dict[0])\n",
    "    current_num_features += num_hw_hp_feats\n",
    "    cw_cp_ind = extract_cw_cp_feat_indices(sample, unigram_dict[1])\n",
    "    unigram_indices = copy.deepcopy(hw_hp_ind)\n",
    "    for item in cw_cp_ind.items():\n",
    "        unigram_indices[current_num_features + item[0]] = item[1]\n",
    "    current_num_features += num_cw_cp_feats\n",
    "    return OrderedDict(sorted(unigram_indices.items(), key=lambda t: t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BIGRAMS\n",
    "\"\"\"\n",
    "\n",
    "def generate_hw_hp_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "            that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, sample[s.head].pos, s.token, s.pos)\n",
    "            if hw_hp_cw_cp_feat_dict.get(feat) is None:\n",
    "                hw_hp_cw_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, head_pos, child_word, child_pos) features: \", current_idx)\n",
    "    hw_hp_cw_cp_feat_dict = OrderedDict(sorted(hw_hp_cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp_cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_cw_cp_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos, child_word, child_pos) features dictionary @ \", path)\n",
    "    return hw_hp_cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_hp_cw_cp_feat_indices(sample, hw_hp_cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_hp_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_hp_cw_cp_dict.get((sample[s.head].token, sample[s.head].pos, s.token, s.pos)):\n",
    "            idx = hw_hp_cw_cp_dict.get((sample[s.head].token, sample[s.head].pos, s.token, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hw_hp_cw_cp_feat_indices_pair(head, child, hw_hp_cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_hp_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hw_hp_cw_cp_dict.get((head.token, head.pos, child.token, child.pos)):\n",
    "        return [hw_hp_cw_cp_dict.get((head.token, head.pos, child.token, child.pos))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hp_cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hp_cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].pos, s.token, s.pos)\n",
    "            if hp_cw_cp_feat_dict.get(feat) is None:\n",
    "                hp_cw_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_pos, child_word, child_pos) features: \", current_idx)\n",
    "    hp_cw_cp_feat_dict = OrderedDict(sorted(hp_cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hp_cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hp_cw_cp_feat_dict, fp)\n",
    "        print(\"saved (head_pos, child_word, child_pos) features dictionary @ \", path)\n",
    "    return hp_cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hp_cw_cp_feat_indices(sample, hp_cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hp_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hp_cw_cp_dict.get((sample[s.head].pos, s.token, s.pos)):\n",
    "            idx = hp_cw_cp_dict.get((sample[s.head].pos, s.token, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hp_cw_cp_feat_indices_pair(head, child, hp_cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hp_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hp_cw_cp_dict.get((head.pos, child.token, child.pos)):\n",
    "        return [hp_cw_cp_dict.get((head.pos, child.token, child.pos))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_hw_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, child_word, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_cw_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_cw_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, s.token, s.pos)\n",
    "            if hw_cw_cp_feat_dict.get(feat) is None:\n",
    "                hw_cw_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, child_word, child_pos) features: \", current_idx)\n",
    "    hw_cw_cp_feat_dict = OrderedDict(sorted(hw_cw_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_cw_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_cw_cp_feat_dict, fp)\n",
    "        print(\"saved (head_word, child_word, child_pos) features dictionary @ \", path)\n",
    "    return hw_cw_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_cw_cp_feat_indices(sample, hw_cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, child_word, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_cw_cp_dict.get((sample[s.head].token, s.token, s.pos)):\n",
    "            idx = hw_cw_cp_dict.get((sample[s.head].token, s.token, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hw_cw_cp_feat_indices_pair(head, child, hw_cw_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, child_word, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hw_cw_cp_dict.get((head.token, child.token, child.pos)):\n",
    "        return [hw_cw_cp_dict.get((head.token, child.token, child.pos))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than \n",
    "                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, sample[s.head].pos, s.pos)\n",
    "            if hw_hp_cp_feat_dict.get(feat) is None:\n",
    "                hw_hp_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, head_pos, child_pos) features: \", current_idx)\n",
    "    hw_hp_cp_feat_dict = OrderedDict(sorted(hw_hp_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_cp_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos, child_pos) features dictionary @ \", path)\n",
    "    return hw_hp_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_hp_cp_feat_indices(sample, hw_hp_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_hp_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_hp_cp_dict.get((sample[s.head].token, sample[s.head].pos, s.pos)):\n",
    "            idx = hw_hp_cp_dict.get((sample[s.head].token, sample[s.head].pos, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hw_hp_cp_feat_indices_pair(head, child, hw_hp_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_hp_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hw_hp_cp_dict.get((head.token, head.pos, child.pos)):\n",
    "        return [hw_hp_cp_dict.get((head.token, head.pos, child.pos))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    \n",
    "def generate_hw_hp_cw_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_word)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than \n",
    "                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_hp_cw_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_hp_cw_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, sample[s.head].pos, s.token)\n",
    "            if hw_hp_cw_feat_dict.get(feat) is None:\n",
    "                hw_hp_cw_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, head_pos, child_word) features: \", current_idx)\n",
    "    hw_hp_cw_feat_dict = OrderedDict(sorted(hw_hp_cw_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_hp_cw.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_hp_cw_feat_dict, fp)\n",
    "        print(\"saved (head_word, head_pos, child_word) features dictionary @ \", path)\n",
    "    return hw_hp_cw_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_hp_cw_feat_indices(sample, hw_hp_cw_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_word)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_hp_cw_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_hp_cw_dict.get((sample[s.head].token, sample[s.head].pos, s.token)):\n",
    "            idx = hw_hp_cw_dict.get((sample[s.head].token, sample[s.head].pos, s.token))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hw_hp_cw_feat_indices_pair(head, child, hw_hp_cw_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_word)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_hp_cw_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hw_hp_cw_dict.get((head.token, head.pos, child.token)):\n",
    "        return [hw_hp_cw_dict.get((head.token, head.pos, child.token))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_hw_cw_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, child_word)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hw_cw_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hw_cw_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].token, s.token)\n",
    "            if hw_cw_feat_dict.get(feat) is None:\n",
    "                hw_cw_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_word, child_word) features: \", current_idx)\n",
    "    hw_cw_feat_dict = OrderedDict(sorted(hw_cw_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hw_cw.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hw_cw_feat_dict, fp)\n",
    "        print(\"saved (head_word, child_word) features dictionary @ \", path)\n",
    "    return hw_cw_feat_dict\n",
    "\n",
    "\n",
    "def extract_hw_cw_feat_indices(sample, hw_cw_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, child_word)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_cw_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hw_cw_dict.get((sample[s.head].token, s.token)):\n",
    "            idx = hw_cw_dict.get((sample[s.head].token, s.token))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hw_cw_feat_indices_pair(head, child, hw_cw_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, child_word)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_cw_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hw_cw_dict.get((head.token, child.token)):\n",
    "        return [hw_cw_dict.get((head.token, child.token))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_hp_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "                    that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: hp_cp_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    samp_gen = dep_sample_generator(path_to_file)\n",
    "    hp_cp_feat_dict = {}\n",
    "    current_idx = 0\n",
    "    for s_i, sample in enumerate(samp_gen):\n",
    "        for s in sample:\n",
    "            if s.token == ROOT or (word_hist.get(s.token) \\\n",
    "                                   and word_hist[s.token] < word_threshold):\n",
    "                continue\n",
    "            feat = (sample[s.head].pos, s.pos)\n",
    "            if hp_cp_feat_dict.get(feat) is None:\n",
    "                hp_cp_feat_dict[feat] = current_idx\n",
    "                current_idx += 1\n",
    "    print(\"total (head_pos, child_pos) features: \", current_idx)\n",
    "    hp_cp_feat_dict = OrderedDict(sorted(hp_cp_feat_dict.items(), key=lambda t: t[1]))\n",
    "    if save_to_file:\n",
    "        path = path_to_file + \".hp_cp.dict\"\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(hp_cp_feat_dict, fp)\n",
    "        print(\"saved (head_pos, child_pos) features dictionary @ \", path)\n",
    "    return hp_cp_feat_dict\n",
    "\n",
    "\n",
    "def extract_hp_cp_feat_indices(sample, hp_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_pos, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: hw_hp_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    feat_indices = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if hp_cp_dict.get((sample[s.head].pos, s.pos)):\n",
    "            idx = hp_cp_dict.get((sample[s.head].pos, s.pos))\n",
    "            if feat_indices.get(idx):\n",
    "                feat_indices[idx] += 1\n",
    "            else:\n",
    "                feat_indices[idx] = 1\n",
    "    return feat_indices\n",
    "\n",
    "\n",
    "def extract_hp_cp_feat_indices_pair(head, child, hp_cp_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_pos, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: hw_hp_cw_cp_dict: the dictionary of indices (dict)\n",
    "    :return: feat_idx: index of the feature (list)\n",
    "    \"\"\"\n",
    "    if hp_cp_dict.get((head.pos, child.pos)):\n",
    "        return [hp_cp_dict.get((head.pos, child.pos))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_bigram_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_word)\n",
    "    * (head_word, child_word)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "                                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: bigram_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    num_features = 0\n",
    "    hw_hp_cw_cp_dict = generate_hw_hp_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                                     save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cw_cp_dict)\n",
    "    hp_cw_cp_dict = generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cw_cp_dict)\n",
    "    hw_cw_cp_dict = generate_hw_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_cw_cp_dict)\n",
    "    hw_hp_cp_dict = generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cp_dict)\n",
    "    hw_hp_cw_dict = generate_hw_hp_cw_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cw_dict)\n",
    "    hw_cw_dict = generate_hw_cw_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_cw_dict)\n",
    "    hp_cp_dict = generate_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cp_dict)\n",
    "    print(\"total bigrams features: \", num_features)\n",
    "    return hw_hp_cw_cp_dict, hp_cw_cp_dict, hw_cw_cp_dict, hw_hp_cp_dict, \\\n",
    "                    hw_hp_cw_dict, hw_cw_dict, hp_cp_dict\n",
    "\n",
    "\n",
    "def update_dict(current_dict, dict_to_add, current_num_features):\n",
    "    \"\"\"\n",
    "    This function takes two dictionaries with indices as keys, and combines them.\n",
    "    :param: current_dict: first dictionary\n",
    "    :param: dict_to_add: second dictionary\n",
    "    :param: current_num_features: total number of features (int)\n",
    "    \"\"\"\n",
    "#     comb_dict = copy.deepcopy(current_dict)\n",
    "    for item in dict_to_add.items():\n",
    "        current_dict[current_num_features + item[0]] = item[1]\n",
    "        \n",
    "def update_list(current_list, list_to_add, current_num_features):\n",
    "    \"\"\"\n",
    "    This function takes two lists with indices, and combines them.\n",
    "    :param: current_list: first list\n",
    "    :param: list_to_add: second list\n",
    "    :param: current_num_features: total number of features (int)\n",
    "    \"\"\"\n",
    "    if list_to_add:\n",
    "        return (current_list + (np.array(list_to_add) + current_num_features).tolist())\n",
    "    else:\n",
    "        return current_list\n",
    "\n",
    "\n",
    "def extract_bigram_feat_indices(sample, bigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_word)\n",
    "    * (head_word, child_word)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: bigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    hw_hp_cw_cp_dict, hp_cw_cp_dict, hw_cw_cp_dict, hw_hp_cp_dict, \\\n",
    "        hw_hp_cw_dict, hw_cw_dict, hp_cp_dict = bigram_dict\n",
    "    \n",
    "    num_hw_hp_cw_cp_feats = len(hw_hp_cw_cp_dict)\n",
    "    num_hp_cw_cp_feats = len(hp_cw_cp_dict)\n",
    "    num_hw_cw_cp_feats = len(hw_cw_cp_dict)\n",
    "    num_hw_hp_cp_feats = len(hw_hp_cp_dict)\n",
    "    num_hw_hp_cw_feats = len(hw_hp_cw_dict)\n",
    "    num_hw_cw_feats = len(hw_cw_dict)\n",
    "    num_hp_cp_feats = len(hp_cp_dict)\n",
    "    \n",
    "    current_num_features = 0\n",
    "    \n",
    "    hw_hp_cw_cp_ind = extract_hw_hp_cw_cp_feat_indices(sample, hw_hp_cw_cp_dict)\n",
    "    current_num_features += num_hw_hp_cw_cp_feats\n",
    "    bigram_indices = copy.deepcopy(hw_hp_cw_cp_ind)\n",
    "    \n",
    "    hp_cw_cp_ind = extract_hp_cw_cp_feat_indices(sample, hp_cw_cp_dict)\n",
    "    update_dict(bigram_indices, hp_cw_cp_ind, current_num_features)\n",
    "    current_num_features += num_hp_cw_cp_feats\n",
    "    \n",
    "    hw_cw_cp_ind = extract_hw_cw_cp_feat_indices(sample, hw_cw_cp_dict)\n",
    "    update_dict(bigram_indices, hw_cw_cp_ind, current_num_features)\n",
    "    current_num_features += num_hw_cw_cp_feats\n",
    "    \n",
    "    hw_hp_cp_ind = extract_hw_hp_cp_feat_indices(sample, hw_hp_cp_dict)\n",
    "    update_dict(bigram_indices, hw_hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hw_hp_cp_feats\n",
    "    \n",
    "    hw_hp_cw_ind = extract_hw_hp_cw_feat_indices(sample, hw_hp_cw_dict)\n",
    "    update_dict(bigram_indices, hw_hp_cw_ind, current_num_features)\n",
    "    current_num_features += num_hw_hp_cw_feats\n",
    "    \n",
    "    hw_cw_ind = extract_hw_cw_feat_indices(sample, hw_cw_dict)\n",
    "    update_dict(bigram_indices, hw_cw_ind, current_num_features)\n",
    "    current_num_features += num_hw_cw_feats\n",
    "    \n",
    "    hp_cp_ind = extract_hp_cp_feat_indices(sample, hp_cp_dict)\n",
    "    update_dict(bigram_indices, hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hp_cp_feats\n",
    "    \n",
    "\n",
    "    return OrderedDict(sorted(bigram_indices.items(), key=lambda t: t[0]))\n",
    "\n",
    "\n",
    "def extract_bigram_feat_indices_pair(head, child, bigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_word)\n",
    "    * (head_word, child_word)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: bigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    hw_hp_cw_cp_dict, hp_cw_cp_dict, hw_cw_cp_dict, hw_hp_cp_dict, \\\n",
    "        hw_hp_cw_dict, hw_cw_dict, hp_cp_dict = bigram_dict\n",
    "    \n",
    "    num_hw_hp_cw_cp_feats = len(hw_hp_cw_cp_dict)\n",
    "    num_hp_cw_cp_feats = len(hp_cw_cp_dict)\n",
    "    num_hw_cw_cp_feats = len(hw_cw_cp_dict)\n",
    "    num_hw_hp_cp_feats = len(hw_hp_cp_dict)\n",
    "    num_hw_hp_cw_feats = len(hw_hp_cw_dict)\n",
    "    num_hw_cw_feats = len(hw_cw_dict)\n",
    "    num_hp_cp_feats = len(hp_cp_dict)\n",
    "    \n",
    "    current_num_features = 0\n",
    "    \n",
    "    hw_hp_cw_cp_ind = extract_hw_hp_cw_cp_feat_indices_pair(head, child, hw_hp_cw_cp_dict)\n",
    "    current_num_features += num_hw_hp_cw_cp_feats\n",
    "    bigram_indices = copy.deepcopy(hw_hp_cw_cp_ind)\n",
    "    \n",
    "    hp_cw_cp_ind = extract_hp_cw_cp_feat_indices_pair(head, child, hp_cw_cp_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hp_cw_cp_ind, current_num_features)\n",
    "    current_num_features += num_hp_cw_cp_feats\n",
    "    \n",
    "    hw_cw_cp_ind = extract_hw_cw_cp_feat_indices_pair(head, child, hw_cw_cp_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hw_cw_cp_ind, current_num_features)\n",
    "    current_num_features += num_hw_cw_cp_feats\n",
    "    \n",
    "    hw_hp_cp_ind = extract_hw_hp_cp_feat_indices_pair(head, child, hw_hp_cp_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hw_hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hw_hp_cp_feats\n",
    "    \n",
    "    hw_hp_cw_ind = extract_hw_hp_cw_feat_indices_pair(head, child, hw_hp_cw_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hw_hp_cw_ind, current_num_features)\n",
    "    current_num_features += num_hw_hp_cw_feats\n",
    "    \n",
    "    hw_cw_ind = extract_hw_cw_feat_indices_pair(head, child, hw_cw_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hw_cw_ind, current_num_features)\n",
    "    current_num_features += num_hw_cw_feats\n",
    "    \n",
    "    hp_cp_ind = extract_hp_cp_feat_indices_pair(head, child, hp_cp_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hp_cp_feats\n",
    "\n",
    "    return sorted(bigram_indices)\n",
    "\n",
    "\n",
    "def generate_bigram_feat_dict_minimal(path_to_file, word_threshold=0, save_to_file=False, word_hist=None):\n",
    "    \"\"\"\n",
    "    This function generates a features dictionary, such that for every features, an index is given.\n",
    "    The following features are generated for a given dataset:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_pos, child_pos)\n",
    "    igram_indices:param: path_to_file: path to location of the dataset (str)\n",
    "    :param: word_threshold: if to consider a feature with word that appears less than\n",
    "                that in the dataset (int)\n",
    "    :param: save_to_file: whether or not to save the dictionary on the disk (bool)\n",
    "    :param: word_hist: dictionary of words histogram in the dataset (dict)\n",
    "    :return: bigram_feat_dict: dictionary feature->index (dict)\n",
    "    \"\"\"\n",
    "    if not word_hist:\n",
    "        word_hist = generate_word_hist_dict(path_to_file)\n",
    "    num_features = 0\n",
    "    hp_cw_cp_dict = generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cw_cp_dict)\n",
    "    hw_hp_cp_dict = generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hw_hp_cp_dict)\n",
    "    \n",
    "    hp_cp_dict = generate_hp_cp_feat_dict(path_to_file, word_threshold=word_threshold,\n",
    "                                               save_to_file=save_to_file, word_hist=word_hist)\n",
    "    num_features += len(hp_cp_dict)\n",
    "    print(\"total bigrams features: \", num_features)\n",
    "    return hp_cw_cp_dict, hw_hp_cp_dict, hp_cp_dict\n",
    "\n",
    "\n",
    "def extract_bigram_feat_indices_minimal(sample, bigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: bigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    hp_cw_cp_dict, hw_hp_cp_dict, hp_cp_dict = bigram_dict\n",
    "\n",
    "    num_hp_cw_cp_feats = len(hp_cw_cp_dict)\n",
    "    num_hw_hp_cp_feats = len(hw_hp_cp_dict)\n",
    "    num_hp_cp_feats = len(hp_cp_dict)\n",
    "    \n",
    "    current_num_features = 0\n",
    "    \n",
    "    hp_cw_cp_ind = extract_hp_cw_cp_feat_indices(sample, hp_cw_cp_dict)\n",
    "    current_num_features += num_hp_cw_cp_feats\n",
    "    bigram_indices = copy.deepcopy(hp_cw_cp_ind)\n",
    "    \n",
    "    hw_hp_cp_ind = extract_hw_hp_cp_feat_indices(sample, hw_hp_cp_dict)\n",
    "    update_dict(bigram_indices, hw_hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hw_hp_cp_feats\n",
    "    \n",
    "    hp_cp_ind = extract_hp_cp_feat_indices(sample, hp_cp_dict)\n",
    "    update_dict(bigram_indices, hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hp_cp_feats\n",
    "\n",
    "    return OrderedDict(sorted(bigram_indices.items(), key=lambda t: t[0]))\n",
    "\n",
    "\n",
    "def extract_bigram_feat_indices_minimal_pair(head, child, bigram_dict):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the bigrams features:\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: bigram_dict: the dictionaries of indices (dict)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    hp_cw_cp_dict, hw_hp_cp_dict, hp_cp_dict = bigram_dict\n",
    "\n",
    "    num_hp_cw_cp_feats = len(hp_cw_cp_dict)\n",
    "    num_hw_hp_cp_feats = len(hw_hp_cp_dict)\n",
    "    num_hp_cp_feats = len(hp_cp_dict)\n",
    "    \n",
    "    current_num_features = 0\n",
    "    \n",
    "    hp_cw_cp_ind = extract_hp_cw_cp_feat_indices_pair(head, child, hp_cw_cp_dict)\n",
    "    bigram_indices = copy.deepcopy(hp_cw_cp_ind)\n",
    "    current_num_features += num_hp_cw_cp_feats\n",
    "    \n",
    "    hw_hp_cp_ind = extract_hw_hp_cp_feat_indices_pair(head, child, hw_hp_cp_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hw_hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hw_hp_cp_feats\n",
    "    \n",
    "    hp_cp_ind = extract_hp_cp_feat_indices_pair(head, child, hp_cp_dict)\n",
    "    bigram_indices = update_list(bigram_indices, hp_cp_ind, current_num_features)\n",
    "    current_num_features += num_hp_cp_feats\n",
    "\n",
    "    return sorted(bigram_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNIGRAMS + BIGRAMS\n",
    "\"\"\"\n",
    "def extract_unigram_bigram_feat_indices(sample, dicts, minimal=False):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_word)\n",
    "    * (head_word, child_word)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: sample: the sample to extract features from (list of DepSample)\n",
    "    :param: dicts: the dictionaries of indices [unigram_dicts, bigrams_dicts] (list)\n",
    "    :param: minimal: whether or not to use the minimal version (bool)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    unigram_dict, bigram_dict = dicts[0], dicts[1]\n",
    "    unigram_inds = extract_unigram_feat_indices(sample, unigram_dict)\n",
    "    feat_indices_dict = copy.deepcopy(unigram_inds)\n",
    "    if minimal:\n",
    "        bigram_inds = extract_bigram_feat_indices_minimal(sample, bigram_dict)\n",
    "    else:\n",
    "        bigram_inds = extract_bigram_feat_indices(sample, bigram_dict)\n",
    "    update_dict(feat_indices_dict, bigram_inds, sum(len(d) for d in unigram_dict))\n",
    "    return OrderedDict(sorted(feat_indices_dict.items(), key=lambda t: t[0]))\n",
    "\n",
    "\n",
    "def extract_unigram_bigram_feat_indices_pair(head, child, dicts, minimal=False):\n",
    "    \"\"\"\n",
    "    This function extracts the indices (in the feature vector) of the features:\n",
    "    * (head_word, head_pos)\n",
    "    * (head_word)\n",
    "    * (head_pos)\n",
    "    * (child_word, child_pos)\n",
    "    * (child_word)\n",
    "    * (child_pos)\n",
    "    * (head_word, head_pos, child_word, child_pos)\n",
    "    * (head_pos, child_word, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_pos)\n",
    "    * (head_word, head_pos, child_word)\n",
    "    * (head_word, child_word)\n",
    "    * (head_pos, child_pos)\n",
    "    :param: head: head DepSample (DepSample)\n",
    "    :param: child: child DepSample (DepSample)\n",
    "    :param: dicts: the dictionaries of indices [unigram_dicts, bigrams_dicts] (list)\n",
    "    :param: minimal: whether or not to use the minimal version (bool)\n",
    "    :return: feat_indices_dict: dictionary idx->count\n",
    "    \"\"\"\n",
    "    unigram_dict, bigram_dict = dicts[0], dicts[1]\n",
    "    unigram_inds = extract_unigram_feat_indices_pair(head, child, unigram_dict)\n",
    "    feat_indices_list = copy.deepcopy(unigram_inds)\n",
    "    if minimal:\n",
    "        bigram_inds = extract_bigram_feat_indices_minimal_pair(head, child, bigram_dict)\n",
    "    else:\n",
    "        bigram_inds = extract_bigram_feat_indices_pair(head, child, bigram_dict)\n",
    "    feat_indices_list = update_list(feat_indices_list, bigram_inds, sum(len(d) for d in unigram_dict))\n",
    "    return sorted(feat_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14162\n"
     ]
    }
   ],
   "source": [
    "word_hist = generate_word_hist_dict(path_to_file)\n",
    "print(len(word_hist))\n",
    "# print(word_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total (head_word, head_pos), (head_word), (head_pos) features:  18896\n",
      "total (child_word, child_pos), (child_word), (child_pos) features:  30104\n",
      "total unigrams features:  49000\n",
      "total (head_word, head_pos, child_word, child_pos) features:  71232\n",
      "total (head_pos, child_word, child_pos) features:  31314\n",
      "total (head_word, child_word, child_pos) features:  70401\n",
      "total (head_word, head_pos, child_pos) features:  33936\n",
      "total (head_word, head_pos, child_word) features:  70679\n",
      "total (head_word, child_word) features:  69819\n",
      "total (head_pos, child_pos) features:  749\n",
      "total bigrams features:  348130\n",
      "total (head_pos, child_word, child_pos) features:  31314\n",
      "total (head_word, head_pos, child_pos) features:  33936\n",
      "total (head_pos, child_pos) features:  749\n",
      "total bigrams features:  65999\n"
     ]
    }
   ],
   "source": [
    "unigram_feat_dict = generate_unigram_feat_dict(path_to_file)\n",
    "# hw_hp_dict = generate_hw_hp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None)\n",
    "# cw_cp_dict = generate_cw_cp_feat_dict(path_to_file, word_threshold=0, save_to_file=False, word_hist=None)\n",
    "# hw_hp_cw_cp_dict = generate_hw_hp_cw_cp_feat_dict(path_to_file, word_threshold=2)\n",
    "# hp_cw_cp_dict = generate_hp_cw_cp_feat_dict(path_to_file, word_threshold=0)\n",
    "# hw_cw_cp_dict = generate_hw_cw_cp_feat_dict(path_to_file, word_threshold=4)\n",
    "# hw_hp_cp_dict = generate_hw_hp_cp_feat_dict(path_to_file, word_threshold=2)\n",
    "# hw_hp_cw_dict = generate_hw_hp_cw_feat_dict(path_to_file, word_threshold=1)\n",
    "# hw_cw_dict = generate_hw_cw_feat_dict(path_to_file, word_threshold=1)\n",
    "# hp_cp_dict = generate_hp_cp_feat_dict(path_to_file, word_threshold=1)\n",
    "bigram_feat_dict = generate_bigram_feat_dict(path_to_file, word_threshold=0)\n",
    "bigram_feat_dict_minimal = generate_bigram_feat_dict_minimal(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4174, 35034, 65274]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_bigram_feat_indices_minimal_pair(s[2], s[4], bigram_feat_dict_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_feat_dict = extract_unigram_bigram_feat_indices(s, [unigram_feat_dict, bigram_feat_dict_minimal],\n",
    "                                                   minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(27, 3),\n",
       "             (432, 3),\n",
       "             (433, 3),\n",
       "             (18911, 1),\n",
       "             (18938, 1),\n",
       "             (18943, 1),\n",
       "             (19142, 1),\n",
       "             (19214, 1),\n",
       "             (19626, 1),\n",
       "             (20197, 1),\n",
       "             (20198, 1),\n",
       "             (20975, 1),\n",
       "             (20976, 1),\n",
       "             (23662, 1),\n",
       "             (23663, 1),\n",
       "             (50439, 1),\n",
       "             (52608, 1),\n",
       "             (53174, 1),\n",
       "             (64983, 1),\n",
       "             (80332, 1),\n",
       "             (84034, 1),\n",
       "             (84081, 1),\n",
       "             (84245, 1),\n",
       "             (114267, 1),\n",
       "             (114274, 1),\n",
       "             (114302, 1),\n",
       "             (114511, 1)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{432: 3, 433: 3, 27: 3}\n",
      "{2079: 1, 2080: 1, 318: 1, 730: 1, 246: 1, 47: 1, 1301: 1, 1302: 1, 15: 1, 4766: 1, 4767: 1, 42: 1}\n",
      "OrderedDict([(27, 3), (432, 3), (433, 3), (18911, 1), (18938, 1), (18943, 1), (19142, 1), (19214, 1), (19626, 1), (20197, 1), (20198, 1), (20975, 1), (20976, 1), (23662, 1), (23663, 1)])\n"
     ]
    }
   ],
   "source": [
    "hw_hp_ind = extract_hw_hp_feat_indices(s, unigram_feat_dict[0])\n",
    "cw_cp_ind = extract_cw_cp_feat_indices(s, unigram_feat_dict[1])\n",
    "unigrams_ind = extract_unigram_feat_indices(s, unigram_feat_dict)\n",
    "print(hw_hp_ind)\n",
    "print(cw_cp_ind)\n",
    "print(unigrams_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 432, 433, 18938, 23662, 23663, 53174, 84034, 114274]\n",
      "[27, 432, 433, 18938, 23662, 23663, 59431, 124406, 161944, 225667, 266285, 336930, 396405]\n"
     ]
    }
   ],
   "source": [
    "# Usage example, how to extract all featires for (head, child)\n",
    "all_features_inds = extract_unigram_bigram_feat_indices_pair(s[2], s[4],\n",
    "                                                             [unigram_feat_dict, bigram_feat_dict_minimal],\n",
    "                                                             minimal=True)\n",
    "print(all_features_inds)\n",
    "\n",
    "all_features_inds = extract_unigram_bigram_feat_indices_pair(s[2], s[4],\n",
    "                                                             [unigram_feat_dict, bigram_feat_dict],\n",
    "                                                             minimal=False)\n",
    "print(all_features_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_successors(sample):\n",
    "    \"\"\"\n",
    "    This function converts sample representation in the form of list of DepSample to\n",
    "    the form of Graph successors: map between heads to list of childs.\n",
    "    :param: sample: the original sample (list of DepSample)\n",
    "    :return: succ_rep: dictionary head->list_of_children (dict)\n",
    "    \"\"\"\n",
    "    succ_rep = {}\n",
    "    for s in sample:\n",
    "        if s.token == ROOT:\n",
    "            continue\n",
    "        if succ_rep.get(s.head) is not None:\n",
    "            succ_rep[s.head].append(s.idx)\n",
    "        else:\n",
    "            succ_rep[s.head] = [s.idx]\n",
    "    return succ_rep\n",
    "\n",
    "\n",
    "def sample_to_full_successors(N):\n",
    "    \"\"\"\n",
    "    This function converts sample representation in the form of list of DepSample to\n",
    "    the form of FULLY CONNECTED Graph successors: map between heads to list of childs.\n",
    "    :param: N: length of the sentence\n",
    "    :return: succ_rep: dictionary head->list_of_children (dict)\n",
    "    \"\"\"\n",
    "    succ_rep = {}\n",
    "    nodes_ids = list(range(N + 1))\n",
    "    for i in nodes_ids:\n",
    "        new_node_ids = copy.deepcopy(nodes_ids)\n",
    "        new_node_ids.remove(0)\n",
    "        if i > 0:\n",
    "            new_node_ids.remove(i)\n",
    "        succ_rep[i] = new_node_ids\n",
    "    return succ_rep\n",
    "\n",
    "\n",
    "def successors_to_sample(sample_no_head, succ_rep):\n",
    "    \"\"\"\n",
    "    This function converts successors representation to list of DepSample.\n",
    "    :param: sample_no_head: list of DepSample where s.head=None\n",
    "    :param: succ_rep: dictionary head->list_of_children (dict)\n",
    "    :return: sample_with_head (list of DepSamples)\n",
    "    \"\"\"\n",
    "    root = DepSample(0, ROOT, ROOT, 0)\n",
    "    sample_with_heads = [root]\n",
    "    for head in succ_rep.keys():\n",
    "        childs = succ_rep[head]\n",
    "        for c in childs:\n",
    "            new_sample = DepSample(sample_no_head[c].idx, sample_no_head[c].token,\n",
    "                                   sample_no_head[c].pos, head)\n",
    "            sample_with_heads.append(new_sample)\n",
    "    return sorted(sample_with_heads, key=lambda t: t.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DepSample(idx=0, token='*', pos='*', head=0), DepSample(idx=1, token='What', pos='WP', head=2), DepSample(idx=2, token=\"'s\", pos='VBZ', head=0), DepSample(idx=3, token='next', pos='JJ', head=2), DepSample(idx=4, token='?', pos='.', head=2)]\n",
      "{2: [1, 3, 4], 0: [2]}\n",
      "[DepSample(idx=0, token='*', pos='*', head=0), DepSample(idx=1, token='What', pos='WP', head=2), DepSample(idx=2, token=\"'s\", pos='VBZ', head=0), DepSample(idx=3, token='next', pos='JJ', head=2), DepSample(idx=4, token='?', pos='.', head=2)]\n"
     ]
    }
   ],
   "source": [
    "succ = sample_to_successors(s)\n",
    "recon_s = successors_to_sample(s, succ)\n",
    "print(s)\n",
    "print(succ)\n",
    "print(recon_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 2, 3, 4, 5, 6, 7, 8],\n",
       " 1: [2, 3, 4, 5, 6, 7, 8],\n",
       " 2: [1, 3, 4, 5, 6, 7, 8],\n",
       " 3: [1, 2, 4, 5, 6, 7, 8],\n",
       " 4: [1, 2, 3, 5, 6, 7, 8],\n",
       " 5: [1, 2, 3, 4, 6, 7, 8],\n",
       " 6: [1, 2, 3, 4, 5, 7, 8],\n",
       " 7: [1, 2, 3, 4, 5, 6, 8],\n",
       " 8: [1, 2, 3, 4, 5, 6, 7]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_to_full_successors(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepOptimizer:\n",
    "    \"\"\"\n",
    "    This helper class holds the weights of a parser model and given a sentence,\n",
    "    calculates scores for edges.\n",
    "    \"\"\"\n",
    "    def __init__(self, w, sample, path_to_train_file=None, dicts=None, feature_extractor=None, minimal=True):\n",
    "        \"\"\"\n",
    "        Initialzie an optimzier.\n",
    "        :param: w: weights (list)\n",
    "        :param: sample: current sample (list of DepSample)\n",
    "        :param: path_to_train_file: training file that contains the samples\n",
    "        :param: dicts: dictionaries of features (list of dicts)\n",
    "        :param: feature_extractor: function to extract feature for (head, child)\n",
    "        :param: minimal: whether or not to use the minimal version of the features\n",
    "        \"\"\"\n",
    "        self.w = w\n",
    "        self.sample = sample\n",
    "        self.minimal= minimal\n",
    "        if path_to_train_file is None:\n",
    "            self.path_to_train_file = './data/train.labeled'\n",
    "        else:\n",
    "            self.path_to_train_file = path_to_train_file\n",
    "        if dicts is None:\n",
    "            if minimal:\n",
    "                self.dicts = [generate_unigram_feat_dict(path_to_file), generate_bigram_feat_dict_minimal(path_to_file)]\n",
    "            else:\n",
    "                self.dicts = [generate_unigram_feat_dict(path_to_file), generate_bigram_feat_dict(path_to_file)]\n",
    "        if feature_extractor is None:\n",
    "            self.feature_extractor = extract_unigram_bigram_feat_indices_pair\n",
    "        else:\n",
    "            self.feature_extractor = feature_extractor\n",
    "    \n",
    "    def get_score(self, head_int, child_int):\n",
    "        \"\"\"\n",
    "        Calculates a score for an edge between `head_int` to `child_int`.\n",
    "        :param: head_int: head node id (int)\n",
    "        :param: child_int: child node id (int)\n",
    "        :return: score: score for the edge (float)\n",
    "        \"\"\"\n",
    "        features_inds = self.feature_extractor(self.sample[head_int], self.sample[child_int], self.dicts, self.minimal)\n",
    "        w = np.array(self.w)\n",
    "        return np.sum(w[features_inds])\n",
    "    \n",
    "    def update_weights(self, w):\n",
    "        \"\"\"\n",
    "        Updates the optimizer current weights.\n",
    "        :param: w: weughts (list)\n",
    "        \"\"\"\n",
    "        self.w = w\n",
    "        \n",
    "    def update_sample(self, sample):\n",
    "        \"\"\"\n",
    "        Upates current sample.\n",
    "        \"\"\"\n",
    "        self.sample = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8481, 11466, 11467, 18943, 19142, 19626]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_unigram_feat_indices_pair(s[1], s[2], unigram_feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([5, 2]) + 3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_comp_file = './data/comp.unlabeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_lines(sample, ref_lines):\n",
    "    \"\"\"\n",
    "    This function generates lines from sample.\n",
    "    :param: sample: list of DepSamples (list)\n",
    "    :param: ref_lines: list of original lines (list of str)\n",
    "    :returns: line\n",
    "    \"\"\"\n",
    "    res_lines = []\n",
    "    for s_i, s in enumerate(sample):\n",
    "        if not s_i:\n",
    "            continue # ROOT\n",
    "        ls = ref_lines[s_i - 1].rstrip().split('\\t')\n",
    "        ls[6] = str(s.head)\n",
    "        res_lines.append('\\t'.join(ls))\n",
    "    return res_lines\n",
    "        \n",
    "\n",
    "def generate_labeled_file(path_to_unlabeled_file):\n",
    "    \"\"\"\n",
    "    This function generates labels for unlabeled samples in the same\n",
    "    format as the original file.\n",
    "    :param: path_to_unlabeled_file: path to loccation of the file (str)\n",
    "    \"\"\"\n",
    "    root = DepSample(0, ROOT, ROOT, 0)\n",
    "    path_to_labeled = path_to_unlabeled_file + '.labeled'\n",
    "    with open(path_to_labeled, 'w') as fw:\n",
    "        with open(path_to_unlabeled_file) as fr:\n",
    "            sample = [root]\n",
    "            lines = []\n",
    "            for line in fr:\n",
    "                if not line.rstrip():\n",
    "                    # end of sample\n",
    "#                     infered_sample = self.infer(sample)\n",
    "                    infered_sample = sample\n",
    "                    res_lines = sample_to_lines(infered_sample, lines)\n",
    "                    for l in res_lines:\n",
    "                        fw.write(l)\n",
    "                        fw.write('\\n')\n",
    "                    fw.write('\\n')\n",
    "                    sample = [root]\n",
    "                    lines = []\n",
    "                else:\n",
    "                    lines.append(line)\n",
    "                    ls = line.rstrip().split('\\t')\n",
    "                    try:\n",
    "                        head = int(ls[6])\n",
    "                    except ValueError:\n",
    "                        head = ls[6]\n",
    "                    sample.append(DepSample(int(ls[0]), ls[1], ls[3], head))\n",
    "    print(\"finished generating labeled file of \", path_to_unlabeled_file, \" @ \", path_to_labeled)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished generating labeled file of  ./data/comp.unlabeled  @  ./data/comp.unlabeled.labeled\n"
     ]
    }
   ],
   "source": [
    "generate_labeled_file(path_to_comp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a = int(\"_\")\n",
    "except ValueError:\n",
    "    a = \"_\"\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
